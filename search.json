[
  {
    "objectID": "data.unpaired.html",
    "href": "data.unpaired.html",
    "title": "Unpaired data loading",
    "section": "",
    "text": "from nbdev.showdoc import *"
  },
  {
    "objectID": "data.unpaired.html#example-dataset---horse-to-zebra-conversion",
    "href": "data.unpaired.html#example-dataset---horse-to-zebra-conversion",
    "title": "Unpaired data loading",
    "section": "Example Dataset - Horse to Zebra conversion",
    "text": "Example Dataset - Horse to Zebra conversion\nHere, we are going to use the horse2zebra dataset provided by UC Berkeley. I have already downloaded it, You can download it at the URL https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip with the fastai untar_data function. Additionally, we can view the directory with Path.ls() (added by fastai).\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n\n\nfolders = horse2zebra.ls().sorted()\nprint(folders)\n\n[Path('/home/tmabraham/.fastai/data/horse2zebra/testA'), Path('/home/tmabraham/.fastai/data/horse2zebra/testB'), Path('/home/tmabraham/.fastai/data/horse2zebra/trainA'), Path('/home/tmabraham/.fastai/data/horse2zebra/trainB')]\n\n\nWe can see that we have four directories, a train and test directory for both domains."
  },
  {
    "objectID": "data.unpaired.html#create-dataloaders-object",
    "href": "data.unpaired.html#create-dataloaders-object",
    "title": "Unpaired data loading",
    "section": "Create DataLoaders object:",
    "text": "Create DataLoaders object:\nWe can treat the image in Domain A as the input and the image in Domain B as the target. We want to be able to index the dataset for a fixed image in domain A but a random image in domain B, in order to avoid fixed pairs.\nA brief summary of how fastai Datasets works: &gt; “A Datasets creates a tuple from items (typically input,target) by applying to them each list of Transform (or Pipeline) in tfms.”\n(from docs)\nSo for transforms we will have a list of list of transforms. Each list of transforms are used to obtain, process, and return the inputs (in this case Domain A) and the targets (Domain B) as a tuple.\nLet’s first get our image paths:\n\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\n\nWe can use get_image_files to get the image files from the directories:\n\nfilesA = get_image_files(trainA_path)\nfilesB = get_image_files(trainB_path)\n\n\nfilesA\n\n(#1067) [Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_6873.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_6335.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_4123.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_4785.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_1798.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_691.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_2048.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_3329.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_835.jpg'),Path('/home/tmabraham/.fastai/data/horse2zebra/trainA/n02381460_993.jpg')...]\n\n\nNow, we can have a Transform that randomly selects an image in domain B for the current pair:\n\nsource\n\nRandPair\n\n RandPair (itemsB)\n\nReturns a random image from domain B, resulting in a random pair of images from domain A and B.\n\ntest_ne(RandPair(filesB)(0),RandPair(filesB)(0))\ntest_eq(type(RandPair(filesB)(0)),type(Path('.')))\n\nNow let’s make our Datasets (assume no split for now). We load as a PILImage, convert to a Tensor, and resize:\n\nsize=128\ndsets = Datasets(filesA, tfms=[[PILImage.create, ToTensor, Resize(size)], \n                               [RandPair(filesB),PILImage.create, ToTensor, Resize(size)]],splits=None)\n\nNow we can create a DataLoader. Note that fastai allows for batch-level transforms that can be performed on an accelerator like a GPU. Let’s normalize the dataset:\n\nbatch_tfms = [IntToFloatTensor, Normalize.from_stats(mean=0.5, std=0.5)]\ndls = dsets.dataloaders(bs=4, num_workers=2, after_batch=batch_tfms)\n\nWe can also show the batch:\n\ndls.show_batch()\n\n\n\n\n\nxb,yb = dls.one_batch()\nxb.shape\n\ntorch.Size([4, 3, 128, 128])\n\n\n\nplt.imshow(dls.after_batch.decode(xb)[0].cpu().permute(1,2,0).numpy())\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\nSome hacks for custom normalization for each of the inputs.\n\nsource\n\n\nTensorImageB\n\n TensorImageB (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nTensorImageA\n\n TensorImageA (x, **kwargs)\n\nA Tensor which support subclass pickling, and maintains metadata when casting or after methods\n\nsource\n\n\nPILImageB\n\n PILImageB ()\n\nA RGB Pillow Image that can show itself and converts to TensorImage\n\nsource\n\n\nPILImageA\n\n PILImageA ()\n\nA RGB Pillow Image that can show itself and converts to TensorImage\n\nsource\n\n\nToTensorB\n\n ToTensorB (enc=None, dec=None, split_idx=None, order=None)\n\nConvert item to TensorImageB\n\nsource\n\n\nToTensorA\n\n ToTensorA (enc=None, dec=None, split_idx=None, order=None)\n\nConvert item to TensorImageA\n\nsource\n\n\nchange_type_of_tfm\n\n change_type_of_tfm (tfm, old_type, new_type)\n\n\ndummy_data = np.ones((128,128,3))\n\n\nnormalize_tfm = Normalize.from_stats(1,1)\n\n\ntest_eq(normalize_tfm(TensorImage(dummy_data).cuda())[0],TensorImage(np.zeros((128,128,3))).cuda())\n\n\nnew_normalize_tfm = change_type_of_tfm(normalize_tfm, TensorImage, TensorImageA)\n\n\ntest_eq(new_normalize_tfm(TensorImageA(dummy_data).cuda())[0],TensorImageA(np.zeros((128,128,3))).cuda())\n\nLet’s add a data loading function to our library. Note that we don’t have a validation set (not necessary for CycleGAN training). Also note that we load the images with size load_size and take a random crop of the image with size crop_size (default of 256x256) to load into the model. We can also specify a subset of the data if we want (num_A and num_B). Finally, we have provided an optional argument to add your own transforms if you need.\n\nsource\n\n\nget_dls\n\n get_dls (pathA, pathB, num_A=None, num_B=None, load_size=512,\n          crop_size=256, item_tfms=None, batch_tfms=None, bs=4,\n          num_workers=2, normalize=False)\n\nGiven image files from two domains (pathA, pathB), create DataLoaders object. Loading and randomly cropped sizes of load_size and crop_size are set to defaults of 512 and 256. Batch size is specified by bs (default=4).\n\n\nQuick tests:\n\nload_size=512\ncrop_size=256\nbs=4\ndls = get_dls(trainA_path, trainB_path,load_size=load_size,crop_size=crop_size,bs=bs)\n\n\ntest_eq(type(dls[0]),TfmdDL)\ntest_eq(len(dls[0]),int(len(trainA_path.ls())/bs))\ntest_eq(len(dls[1]),0)\n\n\nxb,yb = next(iter(dls[0]))\ntest_eq(xb.shape,yb.shape)\ntest_eq(xb.shape,torch.Size([bs, 3, crop_size, crop_size]))\n\n\ndls.show_batch()\n\n\n\n\n\nnum_A = 100\nnum_B = 150\ndls = get_dls(trainA_path, trainB_path,num_A=num_A,num_B=num_B,load_size=load_size,crop_size=crop_size,bs=bs)\n\n\ntest_eq(len(dls[0]),int(min(num_A,num_B)/bs))\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=num_A,num_B=num_B,load_size=load_size,crop_size=crop_size, \n              batch_tfms=[*aug_transforms(size=224), Normalize.from_stats(mean=0.5, std=0.5)], bs=bs)\n\n\ndls.show_batch()\n\n\n\n\n\nclass MakeAll(Transform):\n    order=5\n    def __init__(self, value): self.value = value\n    def encodes(self, x:TensorImage): return TensorImage(torch.ones(*x.shape))*self.value\n    def decodes(self, x:TensorImage): return x\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,item_tfms={TensorImageA: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(xb, TensorImage(torch.ones(*xb.shape)).cuda())\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,item_tfms={TensorImageB: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(yb, TensorImage(torch.ones(*yb.shape)).cuda())\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,item_tfms={TensorImageA: MakeAll(0.), TensorImageB: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(xb, TensorImage(-1*torch.ones(*xb.shape)).cuda())\ntest_eq(yb, TensorImage(torch.ones(*yb.shape)).cuda())\n\n\ndls.show_batch()\n\n\n\n\n\nfrom albumentations import RandomContrast\n\n\nclass AlbumentationsTfm(Transform):\n    order=5\n    def __init__(self, aug): self.aug = aug\n    def encodes(self, img: TensorImage):\n        aug_img = self.aug(image=np.array(img))['image']\n        return type(img)(aug_img)\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,item_tfms={TensorImageA: AlbumentationsTfm(aug=RandomContrast((0.75,0.99),p=1)), \\\n                                                                                           TensorImageB: AlbumentationsTfm(aug=RandomContrast((-0.75,-0.99),p=1))}, bs=bs)\n\n/home/tmabraham/anaconda3/envs/UPIT/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:1826: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  warnings.warn(\n\n\n\ndls.show_batch()\n\n\n\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,batch_tfms={TensorImageA: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(xb, TensorImage(torch.ones(*xb.shape)))\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,batch_tfms={TensorImageB: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(yb, TensorImage(torch.ones(*yb.shape)))\n\n\ndls = get_dls(trainA_path, trainB_path, load_size=load_size,crop_size=crop_size,batch_tfms={TensorImageA: MakeAll(0.), TensorImageB: MakeAll(255.)}, bs=bs)\nxb, yb = next(iter(dls[0]))\ntest_eq(xb, TensorImage(torch.zeros(*xb.shape)))\ntest_eq(yb, TensorImage(torch.ones(*yb.shape)))\n\n\ndls.show_batch()\n\n\n\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=num_A,num_B=num_B,load_size=load_size,crop_size=crop_size, normalize=False, bs=bs)\n\n\ndummy_dl = DataLoader(Datasets([(TensorImageA(torch.ones(3,256,256)),TensorImageB(torch.ones(3,256,256))) for i in range(100)]))\ndummy_dl.to('cuda')\nxb,yb=dls.after_batch(next(iter(dummy_dl)))[0]\ntest_eq(TensorImage(xb),TensorImage(yb))\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=num_A,num_B=num_B,load_size=load_size,crop_size=crop_size, normalize=True, bs=bs)\n\n\ndummy_dl = DataLoader(Datasets([(TensorImageA(torch.ones(3,256,256)),TensorImageB(torch.ones(3,256,256))) for i in range(100)]))\ndummy_dl.to('cuda')\nxb,yb=dls.after_batch(next(iter(dummy_dl)))[0]\ntest_ne(TensorImage(xb),TensorImage(yb))"
  },
  {
    "objectID": "data.unpaired.html#huggingface-datasets-loader",
    "href": "data.unpaired.html#huggingface-datasets-loader",
    "title": "Unpaired data loading",
    "section": "HuggingFace Datasets Loader",
    "text": "HuggingFace Datasets Loader\nHuggingFace has a Datasets package that allows us to access the hundreds of datasets available on the Hub. This includes our Horse-to-Zebra dataset (over here). Here is another helper function specific for creating dataloaders for datasets available on the HuggingFace Hub:\n\nsource\n\nget_dls_from_hf\n\n get_dls_from_hf (dataset_name, fieldA='imageA', fieldB='imageB',\n                  num_A=None, num_B=None, load_size=512, crop_size=256,\n                  item_tfms=None, batch_tfms=None, bs=4, num_workers=2,\n                  normalize=False)\n\nGiven a name of a dataset available on the HuggingFace Hub, create DataLoaders object. Field names given in fieldA and fieldB arguments. Loading and randomly cropped sizes of load_size and crop_size are set to defaults of 512 and 256. Batch size is specified by bs (default=4).\n\nsource\n\n\ncreate_image\n\n create_image (x, image_type)\n\n\nsource\n\n\nconvert_func\n\n convert_func (x)\n\n\n\nQuick tests\n\nload_size=512\ncrop_size=256\nbs=4\ndls = get_dls_from_hf('huggan/horse2zebra',load_size=load_size,crop_size=crop_size,bs=bs)\n\nUsing custom data configuration huggan--horse2zebra-aligned-424fab4179d04c8e\nReusing dataset parquet (/home/tmabraham/.cache/huggingface/datasets/parquet/huggan--horse2zebra-aligned-424fab4179d04c8e/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n\n\n\n\n\n\ntest_eq(type(dls[0]),TfmdDL)\n\n\nxb,yb = next(iter(dls[0]))\ntest_eq(xb.shape,yb.shape)\ntest_eq(xb.shape,torch.Size([bs, 3, crop_size, crop_size]))\n\n\ndls.show_batch()\n\n\n\n\n\nfor i in dls[0]:\n    pass\n\n\ndls = get_dls_from_hf('huggan/horse2zebra',fieldA='imageB',fieldB='imageA',load_size=load_size,crop_size=crop_size,bs=bs)\n\nUsing custom data configuration huggan--horse2zebra-aligned-424fab4179d04c8e\nReusing dataset parquet (/home/tmabraham/.cache/huggingface/datasets/parquet/huggan--horse2zebra-aligned-424fab4179d04c8e/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n\n\n\n\n\n\ndls.show_batch()"
  },
  {
    "objectID": "inference.cyclegan.html",
    "href": "inference.cyclegan.html",
    "title": "CycleGAN batch inference",
    "section": "",
    "text": "If we are given a test set as a folder, we can use the get_preds_cyclegan function defined below to perform batch inference on the images in the folder and save the predictions.\nI found it easier to write my own inference functionality for the custom CycleGAN model than fastai’s built-in functionality.\nI define a PyTorch Dataset that can be used for inference just by passing in the folder with the image files for inference:\n\nsource\n\n\n\n FolderDataset (path, transforms=None)\n\nA PyTorch Dataset class that can be created from a folder path of images, for the sole purpose of inference. Optional transforms can be provided.\nAttributes:\nself.files: A list of the filenames in the folder.\nself.totensor: torchvision.transforms.ToTensor transform.\nself.transform: The transforms passed in as transforms to the constructor.\nLet’s create a helper function for making the DataLoader:\n\nsource\n\n\n\n\n load_dataset (test_path, bs=4, num_workers=4)\n\nA helper function for getting a DataLoader for images in the folder test_path, with batch size bs, and number of workers num_workers\n\nsource\n\n\n\n\n get_preds_cyclegan (learn, test_path, pred_path, convert_to='B', bs=4,\n                     num_workers=4, device='cuda', suffix='tif')\n\nA prediction function that takes the Learner object learn with the trained model, the test_path folder with the images to perform batch inference on, and the output folder pred_path where the predictions will be saved. The function will convert images to the domain specified by convert_to (default is ‘B’). The other arguments are the batch size bs (default=4), num_workers (default=4), the device to run inference on (default=‘cuda’) and suffix of the prediction images suffix (default=‘tif’).\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n\n\n    \n        \n      \n      100.01% [116875264/116867962 00:22&lt;00:00]\n    \n    \n\n\n\nfolders = horse2zebra.ls().sorted()\n\n\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\n\n\ndls = get_dls(trainA_path, trainB_path,load_size=286)\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan)\nlearn.model_dir = '.'\nlearn = learn.load('models/model')\n\n/usr/local/lib/python3.8/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n\n\n\npreds_path = './h2z-preds'\nget_preds_cyclegan(learn,str(testA_path),preds_path,bs=1,device='cpu')\n\n\n    \n        \n      \n      100.00% [120/120 00:48&lt;00:00]\n    \n    \n\n\n\npreds_path = './h2z-preds'\nget_preds_cyclegan(learn,str(testA_path),preds_path)\n\n\n    \n        \n      \n      100.00% [30/30 00:10&lt;00:00]\n    \n    \n\n\n\nImage.open(testA_path.ls()[100])\n\n\n\n\n\nImage.open(os.path.join(preds_path,testA_path.ls()[100].parts[-1][:-4]+'_fakeB.tif'))\n\n\n\n\n\npreds_path = './z2h-preds'\nget_preds_cyclegan(learn,str(testB_path),preds_path,convert_to='A')\n\n\n    \n        \n      \n      100.00% [35/35 00:09&lt;00:00]\n    \n    \n\n\n\nImage.open(testB_path.ls()[100])\n\n\n\n\n\nImage.open(os.path.join(preds_path,testB_path.ls()[100].parts[-1][:-4]+'_fakeA.tif'))"
  },
  {
    "objectID": "inference.cyclegan.html#batch-inference-functionality",
    "href": "inference.cyclegan.html#batch-inference-functionality",
    "title": "CycleGAN batch inference",
    "section": "",
    "text": "If we are given a test set as a folder, we can use the get_preds_cyclegan function defined below to perform batch inference on the images in the folder and save the predictions.\nI found it easier to write my own inference functionality for the custom CycleGAN model than fastai’s built-in functionality.\nI define a PyTorch Dataset that can be used for inference just by passing in the folder with the image files for inference:\n\nsource\n\n\n\n FolderDataset (path, transforms=None)\n\nA PyTorch Dataset class that can be created from a folder path of images, for the sole purpose of inference. Optional transforms can be provided.\nAttributes:\nself.files: A list of the filenames in the folder.\nself.totensor: torchvision.transforms.ToTensor transform.\nself.transform: The transforms passed in as transforms to the constructor.\nLet’s create a helper function for making the DataLoader:\n\nsource\n\n\n\n\n load_dataset (test_path, bs=4, num_workers=4)\n\nA helper function for getting a DataLoader for images in the folder test_path, with batch size bs, and number of workers num_workers\n\nsource\n\n\n\n\n get_preds_cyclegan (learn, test_path, pred_path, convert_to='B', bs=4,\n                     num_workers=4, device='cuda', suffix='tif')\n\nA prediction function that takes the Learner object learn with the trained model, the test_path folder with the images to perform batch inference on, and the output folder pred_path where the predictions will be saved. The function will convert images to the domain specified by convert_to (default is ‘B’). The other arguments are the batch size bs (default=4), num_workers (default=4), the device to run inference on (default=‘cuda’) and suffix of the prediction images suffix (default=‘tif’).\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n\n\n    \n        \n      \n      100.01% [116875264/116867962 00:22&lt;00:00]\n    \n    \n\n\n\nfolders = horse2zebra.ls().sorted()\n\n\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\n\n\ndls = get_dls(trainA_path, trainB_path,load_size=286)\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan)\nlearn.model_dir = '.'\nlearn = learn.load('models/model')\n\n/usr/local/lib/python3.8/dist-packages/fastai/learner.py:56: UserWarning: Saved filed doesn't contain an optimizer state.\n  elif with_opt: warn(\"Saved filed doesn't contain an optimizer state.\")\n\n\n\npreds_path = './h2z-preds'\nget_preds_cyclegan(learn,str(testA_path),preds_path,bs=1,device='cpu')\n\n\n    \n        \n      \n      100.00% [120/120 00:48&lt;00:00]\n    \n    \n\n\n\npreds_path = './h2z-preds'\nget_preds_cyclegan(learn,str(testA_path),preds_path)\n\n\n    \n        \n      \n      100.00% [30/30 00:10&lt;00:00]\n    \n    \n\n\n\nImage.open(testA_path.ls()[100])\n\n\n\n\n\nImage.open(os.path.join(preds_path,testA_path.ls()[100].parts[-1][:-4]+'_fakeB.tif'))\n\n\n\n\n\npreds_path = './z2h-preds'\nget_preds_cyclegan(learn,str(testB_path),preds_path,convert_to='A')\n\n\n    \n        \n      \n      100.00% [35/35 00:09&lt;00:00]\n    \n    \n\n\n\nImage.open(testB_path.ls()[100])\n\n\n\n\n\nImage.open(os.path.join(preds_path,testB_path.ls()[100].parts[-1][:-4]+'_fakeA.tif'))"
  },
  {
    "objectID": "tracking.wandb.html",
    "href": "tracking.wandb.html",
    "title": "Weights and Biases Callback",
    "section": "",
    "text": "source\n\nSaveModelAtEndCallback\n\n SaveModelAtEndCallback (fname='model', with_opt=False)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\nsource\n\n\nlog_dataset\n\n log_dataset (main_path, folder_names=None, name=None, metadata={},\n              description='raw dataset')\n\nLog dataset folder\n\nsource\n\n\nUPITWandbCallback\n\n UPITWandbCallback (log='gradients', log_preds=True, log_model=True,\n                    log_dataset=False, folder_names=None,\n                    dataset_name=None, valid_dl=None, n_preds=36,\n                    seed=12345, reorder=True)\n\nSaves model topology, losses & metrics\n\nimport tempfile\n\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\nfolders = horse2zebra.ls().sorted()\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\ndls = get_dls(trainA_path, trainB_path, num_A=100, num_B=100, load_size=286)\n\n#os.environ['WANDB_MODE'] = 'dryrun' # run offline\nwandb.init()\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan,opt_func=partial(Adam,mom=0.5,sqr_mom=0.999),\n                    cbs=[UPITWandbCallback(log_preds=True, log_model=True, log_dataset=horse2zebra, folder_names=[trainA_path.name,trainB_path.name])],\n                    metrics=[FrechetInceptionDistance()])\n\nlearn.fit_flat_lin(1,1,2e-4)\nwandb.finish()\n\nwandb: Currently logged in as: tmabraham. Use `wandb login --relogin` to force relogin\nwandb: Adding directory to artifact (/home/tmabraham/.fastai/data/horse2zebra/trainA)... Done. 0.3s\nwandb: Adding directory to artifact (/home/tmabraham/.fastai/data/horse2zebra/trainB)... Done. 0.1s\n/home/tmabraham/anaconda3/envs/UPIT/lib/python3.9/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n  warn(\"Your generator is empty.\")\n\n\nwandb version 0.12.21 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.17\n\n\nRun data is saved locally in /home/tmabraham/UPIT/nbs/wandb/run-20220801_234826-umaf3jfa\n\n\nSyncing run floral-voice-43 to Weights & Biases (docs)\n\n\nCould not gather input dimensions\nAdding SaveModelAtEndCallback()\nSaving training set predictions\nWandbCallback was not able to get prediction samples -&gt; To be implemented\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nid_loss_A\nid_loss_B\ngen_loss_A\ngen_loss_B\ncyc_loss_A\ncyc_loss_B\nD_A_loss\nD_B_loss\nfrechet_inception_distance\ntime\n\n\n\n\n0\n9.819530\n1.455294\n1.471061\n0.432893\n0.477370\n3.085176\n3.139654\n0.405227\n0.405442\n93.815806\n00:17\n\n\n1\n8.568236\n1.112963\n1.203730\n0.294678\n0.299053\n2.385156\n2.604889\n0.257748\n0.254994\n93.795887\n00:16\n\n\n\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\nD_A_loss\n█▁\n\n\nD_B_loss\n█▁\n\n\ncyc_loss_A\n█▁\n\n\ncyc_loss_B\n█▁\n\n\nepoch\n▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n\n\neps_0\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nfrechet_inception_distance\n█▁\n\n\ngen_loss_A\n█▁\n\n\ngen_loss_B\n█▁\n\n\nid_loss_A\n█▁\n\n\nid_loss_B\n█▁\n\n\nlr_0\n██████████████████████▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁\n\n\nmom_0\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nraw_loss\n█▇▆▅▄▄▃▃▄▂▂▂▃▄▂▃▂▂▃▂▃▂▂▃▂▂▂▃▂▃▂▂▃▂▁▂▂▂▁▂\n\n\nsqr_mom_0\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\ntrain_loss\n█▇▆▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n\n\nwd_0\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\nD_A_loss\n0.25775\n\n\nD_B_loss\n0.25499\n\n\ncyc_loss_A\n2.38516\n\n\ncyc_loss_B\n2.60489\n\n\nepoch\n2\n\n\neps_0\n1e-05\n\n\nfrechet_inception_distance\n93.79589\n\n\ngen_loss_A\n0.29468\n\n\ngen_loss_B\n0.29905\n\n\nid_loss_A\n1.11296\n\n\nid_loss_B\n1.20373\n\n\nlr_0\n1e-05\n\n\nmom_0\n0.5\n\n\nraw_loss\n7.13516\n\n\nsqr_mom_0\n0.999\n\n\ntrain_loss\n8.56824\n\n\nwd_0\n0.01\n\n\n\n\n\n\n\nSynced floral-voice-43: https://wandb.ai/tmabraham/UPIT-nbs/runs/umaf3jfaSynced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)\n\n\nFind logs at: ./wandb/run-20220801_234826-umaf3jfa/logs\n\n\n\nb = dls.one_batch()\n_,_,preds = learn.get_preds(dl=[b], with_decoded=True)\n\n\n\n\n\n\n\n\n\ndls.show_batch((b[0], b[1]), max_n=2, show=True)\nplt.suptitle('Input')\ndls.show_batch((preds[1],preds[0]), max_n=2, show=True)\nplt.suptitle('Predictions')\n\nText(0.5, 0.98, 'Predictions')"
  },
  {
    "objectID": "models.ganilla.html",
    "href": "models.ganilla.html",
    "title": "GANILLA model",
    "section": "",
    "text": "We use the generator that was introduced in the GANILLA paper."
  },
  {
    "objectID": "models.ganilla.html#generator",
    "href": "models.ganilla.html#generator",
    "title": "GANILLA model",
    "section": "Generator",
    "text": "Generator\n\nsource\n\nBasicBlock_Ganilla\n\n BasicBlock_Ganilla (in_planes, planes, use_dropout, stride=1)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nPyramidFeatures\n\n PyramidFeatures (C2_size, C3_size, C4_size, C5_size, fpn_weights,\n                  feature_size=128)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nResNet\n\n ResNet (input_nc, output_nc, ngf, use_dropout, fpn_weights, block,\n         layers)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\ninit_weights\n\n init_weights (net, init_type='normal', gain=0.02)\n\n\nsource\n\n\nganilla_generator\n\n ganilla_generator (input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0,\n                    1.0, 1.0], init_type='normal', gain=0.02, **kwargs)\n\nConstructs a ResNet-18 GANILLA generator.\nLet’s test for a few things: 1. The generator can indeed be initialized correctly 2. A random image can be passed into the model successfully with the correct size output\nFirst let’s create a random batch:\n\nimg1 = torch.randn(4,3,256,256)\n\n\nm = ganilla_generator(3,3,64,0.5)\nwith torch.no_grad():\n    out1 = m(img1)\nout1.shape\n\ntorch.Size([4, 3, 256, 256])"
  },
  {
    "objectID": "models.ganilla.html#full-model",
    "href": "models.ganilla.html#full-model",
    "title": "GANILLA model",
    "section": "Full model",
    "text": "Full model\nWe group two discriminators and two generators in a single model, then a Callback (defined in 02_cyclegan_training.ipynb) will take care of training them properly. The discriminator and training loop is the same as CycleGAN.\n\nsource\n\nGANILLA\n\n GANILLA (ch_in:int=3, ch_out:int=3, n_features:int=64, disc_layers:int=3,\n          lsgan:bool=True, drop:float=0.0,\n          norm_layer:torch.nn.modules.module.Module=None,\n          fpn_weights:list=[1.0, 1.0, 1.0, 1.0], init_type:str='normal',\n          gain:float=0.02, **kwargs)\n\nGANILLA model.\nWhen called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\nAttributes:\nG_A (nn.Module): takes real input B and generates fake input A\nG_B (nn.Module): takes real input A and generates fake input B\nD_A (nn.Module): trained to make the difference between real input A and fake input A\nD_B (nn.Module): trained to make the difference between real input B and fake input B\n\nsource\n\n\nGANILLA.__init__\n\n GANILLA.__init__ (ch_in:int=3, ch_out:int=3, n_features:int=64,\n                   disc_layers:int=3, lsgan:bool=True, drop:float=0.0,\n                   norm_layer:torch.nn.modules.module.Module=None,\n                   fpn_weights:list=[1.0, 1.0, 1.0, 1.0],\n                   init_type:str='normal', gain:float=0.02, **kwargs)\n\nConstructor for GANILLA model.\nArguments:\nch_in (int): Number of input channels (default=3)\nch_out (int): Number of output channels (default=3)\nn_features (int): Number of input features (default=64)\ndisc_layers (int): Number of discriminator layers (default=3)\nlsgan (bool): LSGAN training objective (output unnormalized float) or not? (default=True)\ndrop (float): Level of dropout (default=0)\nnorm_layer (nn.Module): Type of normalization layer to use in the discriminator (default=None) fpn_weights (list): Weights for feature pyramid network (default=[1.0, 1.0, 1.0, 1.0])\ninit_type (str): Type of initialization (default=‘normal’)\ngain (float): Gain for initialization (default=0.02)\n\nsource\n\n\nGANILLA.forward\n\n GANILLA.forward (input)\n\nForward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B.\n\n\nQuick model tests\nAgain, let’s check that the model can be called sucsessfully and outputs the correct shapes.\n\nganilla_model = GANILLA()\nimg1 = torch.randn(4,3,256,256)\nimg2 = torch.randn(4,3,256,256)\n\n\nwith torch.no_grad(): ganilla_output = ganilla_model((img1,img2))\n\nCPU times: user 42.1 s, sys: 8.36 s, total: 50.5 s\nWall time: 1.38 s\n\n\n\ntest_eq(len(ganilla_output),4)\nfor output_batch in ganilla_output:\n    test_eq(output_batch.shape,img1.shape)\n\n\nganilla_model.push_to_hub('upit-ganilla-test')\n\nCloning https://huggingface.co/tmabraham/upit-ganilla-test into local empty directory.\nTo https://huggingface.co/tmabraham/upit-ganilla-test\n   264c8d0..38cafb3  main -&gt; main\n\n\n\n\n\n\n'https://huggingface.co/tmabraham/upit-ganilla-test/commit/38cafb3d4cca069313b8ed03d88ecc88db28f3d5'\n\n\n\nganilla_model.from_pretrained('tmabraham/upit-ganilla-test')"
  },
  {
    "objectID": "models.cyclegan.html",
    "href": "models.cyclegan.html",
    "title": "CycleGAN model",
    "section": "",
    "text": "We use the models that were introduced in the cycleGAN paper."
  },
  {
    "objectID": "models.cyclegan.html#generator",
    "href": "models.cyclegan.html#generator",
    "title": "CycleGAN model",
    "section": "Generator",
    "text": "Generator\n\nsource\n\nconvT_norm_relu\n\n convT_norm_relu (ch_in:int, ch_out:int,\n                  norm_layer:torch.nn.modules.module.Module, ks:int=3,\n                  stride:int=2, bias:bool=True)\n\n\nsource\n\n\npad_conv_norm_relu\n\n pad_conv_norm_relu (ch_in:int, ch_out:int, pad_mode:str,\n                     norm_layer:torch.nn.modules.module.Module, ks:int=3,\n                     bias:bool=True, pad=1, stride:int=1, activ:bool=True,\n                     init=&lt;function kaiming_normal_&gt;, init_gain:int=0.02)\n\n\nsource\n\n\nResnetBlock\n\n ResnetBlock (dim:int, pad_mode:str='reflection',\n              norm_layer:torch.nn.modules.module.Module=None,\n              dropout:float=0.0, bias:bool=True)\n\nnn.Module for the ResNet Block\n\nsource\n\n\nresnet_generator\n\n resnet_generator (ch_in:int, ch_out:int, n_ftrs:int=64,\n                   norm_layer:torch.nn.modules.module.Module=None,\n                   dropout:float=0.0, n_blocks:int=9,\n                   pad_mode:str='reflection')\n\n\n\nTest generator\nLet’s test for a few things: 1. The generator can indeed be initialized correctly 2. A random image can be passed into the model successfully with the correct size output 3. The CycleGAN generator is equivalent to the original implementation\nFirst let’s create a random batch:\n\nimg1 = torch.randn(4,3,256,256)\n\n\nm = resnet_generator(3,3)\nwith torch.no_grad():\n    out1 = m(img1)\nout1.shape\n\ntorch.Size([4, 3, 256, 256])\n\n\n\nm_junyanz = define_G(3,3,64,'resnet_9blocks', norm='instance')\nwith torch.no_grad():\n    out2 = m_junyanz(img1)\nout2.shape\n\ninitialize network with normal\n\n\ntorch.Size([4, 3, 256, 256])\n\n\n\nsource\n\n\ncompare_networks\n\n compare_networks (a, b)\n\nA simple function to compare the printed model representations as a proxy for actually comparing two models\n\ntest_eq(out1.shape,img1.shape)\ntest_eq(out2.shape,img1.shape)\nassert compare_networks(list(m_junyanz.children())[0],m)\n\nPassed!"
  },
  {
    "objectID": "models.cyclegan.html#discriminator",
    "href": "models.cyclegan.html#discriminator",
    "title": "CycleGAN model",
    "section": "Discriminator",
    "text": "Discriminator\n\nsource\n\nconv_norm_lr\n\n conv_norm_lr (ch_in:int, ch_out:int,\n               norm_layer:torch.nn.modules.module.Module=None, ks:int=3,\n               bias:bool=True, pad:int=1, stride:int=1, activ:bool=True,\n               slope:float=0.2, init=&lt;function normal_&gt;,\n               init_gain:int=0.02)\n\n\nsource\n\n\ndiscriminator\n\n discriminator (ch_in:int, n_ftrs:int=64, n_layers:int=3,\n                norm_layer:torch.nn.modules.module.Module=None,\n                sigmoid:bool=False)\n\n\n\nTest discriminator\nLet’s test for similar things: 1. The discriminator can indeed be initialized correctly 2. A random image can be passed into the discriminator successfully with the correct size output 3. The CycleGAN discriminator is equivalent to the original implementation\n\nd = discriminator(3)\nwith torch.no_grad():\n    out1 = d(img1)\nout1.shape\n\ntorch.Size([4, 1, 30, 30])\n\n\n\nimg1 = torch.randn(4,3,256,256)\n\n\nd_junyanz = define_D(3,64,'basic',norm='instance')\nwith torch.no_grad():\n    out2 = d_junyanz(img1)\nout2.shape\n\ninitialize network with normal\n\n\ntorch.Size([4, 1, 30, 30])\n\n\n\ntest_eq(out1.shape,torch.Size([4, 1, 30, 30]))\ntest_eq(out2.shape,torch.Size([4, 1, 30, 30]))\nassert compare_networks(list(d_junyanz.children())[0],d)\n\nPassed!"
  },
  {
    "objectID": "models.cyclegan.html#full-model",
    "href": "models.cyclegan.html#full-model",
    "title": "CycleGAN model",
    "section": "Full model",
    "text": "Full model\nWe group two discriminators and two generators in a single model, then a Callback (defined in 02_cyclegan_training.ipynb) will take care of training them properly. We use the PyTorchModelHubMixin to provide support for pushing to and loading from the HuggingFace Hub.\n\nsource\n\nCycleGAN\n\n CycleGAN (ch_in:int=3, ch_out:int=3, n_features:int=64,\n           disc_layers:int=3, gen_blocks:int=9, lsgan:bool=True,\n           drop:float=0.0, norm_layer:torch.nn.modules.module.Module=None)\n\nCycleGAN model.\nWhen called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\nAttributes:\nG_A (nn.Module): takes real input B and generates fake input A\nG_B (nn.Module): takes real input A and generates fake input B\nD_A (nn.Module): trained to make the difference between real input A and fake input A\nD_B (nn.Module): trained to make the difference between real input B and fake input B\n\nsource\n\n\nCycleGAN.__init__\n\n CycleGAN.__init__ (ch_in:int=3, ch_out:int=3, n_features:int=64,\n                    disc_layers:int=3, gen_blocks:int=9, lsgan:bool=True,\n                    drop:float=0.0,\n                    norm_layer:torch.nn.modules.module.Module=None)\n\nConstructor for CycleGAN model.\nArguments:\nch_in (int): Number of input channels (default=3)\nch_out (int): Number of output channels (default=3)\nn_features (int): Number of input features (default=64)\ndisc_layers (int): Number of discriminator layers (default=3)\ngen_blocks (int): Number of residual blocks in the generator (default=9)\nlsgan (bool): LSGAN training objective (output unnormalized float) or not? (default=True)\ndrop (float): Level of dropout (default=0)\nnorm_layer (nn.Module): Type of normalization layer to use in the models (default=None)\n\nsource\n\n\nCycleGAN.forward\n\n CycleGAN.forward (input)\n\nForward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B.\n\n\n\nModelHubMixin.push_to_hub\n\n ModelHubMixin.push_to_hub (repo_id:str, config:Optional[dict]=None,\n                            commit_message:str='Push model using\n                            huggingface_hub.', private:bool=False,\n                            api_endpoint:Optional[str]=None,\n                            token:Optional[str]=None,\n                            branch:Optional[str]=None,\n                            create_pr:Optional[bool]=None, allow_patterns:\n                            Union[List[str],str,NoneType]=None, ignore_pat\n                            terns:Union[List[str],str,NoneType]=None, dele\n                            te_patterns:Union[List[str],str,NoneType]=None\n                            )\n\nUpload model checkpoint to the Hub.\nUse allow_patterns and ignore_patterns to precisely filter which files should be pushed to the hub. Use delete_patterns to delete existing remote files in the same commit. See [upload_folder] reference for more details.\nArgs: repo_id (str): ID of the repository to push to (example: \"username/my-model\"). config (dict, optional): Configuration object to be saved alongside the model weights. commit_message (str, optional): Message to commit while pushing. private (bool, optional, defaults to False): Whether the repository created should be private. api_endpoint (str, optional): The API endpoint to use when pushing the model to the hub. token (str, optional): The token to use as HTTP bearer authorization for remote files. By default, it will use the token cached when running huggingface-cli login. branch (str, optional): The git branch on which to push the model. This defaults to \"main\". create_pr (boolean, optional): Whether or not to create a Pull Request from branch with that commit. Defaults to False. allow_patterns (List[str] or str, optional): If provided, only files matching at least one pattern are pushed. ignore_patterns (List[str] or str, optional): If provided, files matching any of the patterns are not pushed. delete_patterns (List[str] or str, optional): If provided, remote files matching any of the patterns will be deleted from the repo.\nReturns: The url of the commit of your model in the given repository.\n\n\n\nModelHubMixin.from_pretrained\n\n ModelHubMixin.from_pretrained (cls:Type[~T],\n                                pretrained_model_name_or_path:Union[str,pa\n                                thlib.Path], force_download:bool=False,\n                                resume_download:bool=False,\n                                proxies:Optional[Dict]=None,\n                                token:Union[bool,str,NoneType]=None, cache\n                                _dir:Union[pathlib.Path,str,NoneType]=None\n                                , local_files_only:bool=False,\n                                revision:Optional[str]=None,\n                                **model_kwargs)\n\nDownload a model from the Huggingface Hub and instantiate it.\nArgs: pretrained_model_name_or_path (str, Path): - Either the model_id (string) of a model hosted on the Hub, e.g. bigscience/bloom. - Or a path to a directory containing model weights saved using [~transformers.PreTrainedModel.save_pretrained], e.g., ../path/to/my_model_directory/. revision (str, optional): Revision of the model on the Hub. Can be a branch name, a git tag or any commit id. Defaults to the latest commit on main branch. force_download (bool, optional, defaults to False): Whether to force (re-)downloading the model weights and configuration files from the Hub, overriding the existing cache. resume_download (bool, optional, defaults to False): Whether to delete incompletely received files. Will attempt to resume the download if such a file exists. proxies (Dict[str, str], optional): A dictionary of proxy servers to use by protocol or endpoint, e.g., {'http': 'foo.bar:3128',         'http://hostname': 'foo.bar:4012'}. The proxies are used on every request. token (str or bool, optional): The token to use as HTTP bearer authorization for remote files. By default, it will use the token cached when running huggingface-cli login. cache_dir (str, Path, optional): Path to the folder where cached files are stored. local_files_only (bool, optional, defaults to False): If True, avoid downloading the file and return the path to the local cached file if it exists. model_kwargs (Dict, optional): Additional kwargs to pass to the model during initialization.\n\n\nQuick model tests\nAgain, let’s check that the model can be called sucsessfully and outputs the correct shapes.\n\ncyclegan_model = CycleGAN()\nimg1 = torch.randn(4,3,256,256)\nimg2 = torch.randn(4,3,256,256)\n\n\nwith torch.no_grad(): cyclegan_output = cyclegan_model((img1,img2))\n\nCPU times: user 1min 15s, sys: 6.67 s, total: 1min 22s\nWall time: 2.25 s\n\n\n\ntest_eq(len(cyclegan_output),4)\nfor output_batch in cyclegan_output:\n    test_eq(output_batch.shape,img1.shape)\n\n\ncyclegan_model.push_to_hub('upit-cyclegan-test')\n\nCloning https://huggingface.co/tmabraham/upit-cyclegan-test into local empty directory.\nTo https://huggingface.co/tmabraham/upit-cyclegan-test\n   a41e9e0..2331f7d  main -&gt; main\n\n\n\n\n\n\n'https://huggingface.co/tmabraham/upit-cyclegan-test/commit/2331f7d345d719ac1fdfb10b2cddf58abd7931bb'\n\n\n\ncyclegan_model.from_pretrained('tmabraham/upit-cyclegan-test')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unpaired image-to-image translation",
    "section": "",
    "text": "This is a package for training and testing unpaired image-to-image translation models. It currently only includes the CycleGAN, DualGAN, and GANILLA models, but other models will be implemented in the future.\nThis package uses fastai to accelerate deep learning experimentation. Additionally, nbdev was used to develop the package and produce documentation based on a series of notebooks."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Unpaired image-to-image translation",
    "section": "Install",
    "text": "Install\nTo install, use pip:\npip install git+https://github.com/tmabraham/UPIT.git\nThe package uses torch 1.7.1, torchvision 0.8.2, and fastai 2.3.0 (and its dependencies). It also requires nbdev 1.1.13 if you would like to add features to the package. Finally, for creating a web app model interface, gradio 1.1.6 is used."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Unpaired image-to-image translation",
    "section": "How to use",
    "text": "How to use\nTraining a CycleGAN model is easy with UPIT! Given the paths of the images from the two domains trainA_path and trainB_path, you can do the following:\n\nfrom upit.data.unpaired import *\nfrom upit.models.cyclegan import *\nfrom upit.train.cyclegan import *\n\n\ndls = get_dls(trainA_path, trainB_path)\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan,opt_func=partial(Adam,mom=0.5,sqr_mom=0.999))\nlearn.fit_flat_lin(100,100,2e-4)\n\nThe GANILLA model is only a different generator model architecture (that’s meant to strike a better balance between style and content), so the same cycle_learner class can be used.\n\nfrom upit.models.ganilla import *\n\n\nganilla = GANILLA(3,3,64)\nlearn = cycle_learner(dls, ganilla,opt_func=partial(Adam,mom=0.5,sqr_mom=0.999))\nlearn.fit_flat_lin(100,100,2e-4)\n\nFinally, we provide separate functions/classes for DualGAN model and training:\n\nfrom upit.models.dualgan import *\nfrom upit.train.dualgan import *\n\n\ndual_gan = DualGAN(3,64,3)\nlearn = dual_learner(dls, dual_gan, opt_func=RMSProp)\nlearn.fit_flat_lin(100,100,2e-4)\n\nAdditionally, we provide metrics for quantitative evaluation of the models, as well as experiment tracking with Weights and Biases. Check the documentation for more information!"
  },
  {
    "objectID": "index.html#citing-upit",
    "href": "index.html#citing-upit",
    "title": "Unpaired image-to-image translation",
    "section": "Citing UPIT",
    "text": "Citing UPIT\nIf you use UPIT in your research please use the following BibTeX entry:\n@Misc{UPIT,\n    author =       {Tanishq Mathew Abraham},\n    title =        {UPIT - A fastai/PyTorch package for unpaired image-to-image translation.},\n    howpublished = {Github},\n    year =         {2021},\n    url =          {https://github.com/tmabraham/UPIT}\n}"
  },
  {
    "objectID": "train.dualgan.html",
    "href": "train.dualgan.html",
    "title": "DualGAN training loop",
    "section": "",
    "text": "set_seed(999, reproducible=True)\nsource"
  },
  {
    "objectID": "train.dualgan.html#quick-test",
    "href": "train.dualgan.html#quick-test",
    "title": "DualGAN training loop",
    "section": "Quick Test",
    "text": "Quick Test\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n\n\nfolders = horse2zebra.ls().sorted()\n\n\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=100)\n\n\ndual_gan = DualGAN()\nlearn = dual_learner(dls, dual_gan,show_img_interval=1)\n\n\nlearn.show_training_loop()\n\nStart Fit\n   - before_fit     : [TrainEvalCallback, ShowImgsCallback, Recorder, ProgressCallback]\n  Start Epoch Loop\n     - before_epoch   : [Recorder, ProgressCallback]\n    Start Train\n       - before_train   : [TrainEvalCallback, DualGANTrainer, Recorder, ProgressCallback]\n      Start Batch Loop\n         - before_batch   : [DualGANTrainer]\n         - after_pred     : [DualGANTrainer]\n         - after_loss     : []\n         - before_backward: []\n         - before_step    : []\n         - after_step     : []\n         - after_cancel_batch: []\n         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n      End Batch Loop\n    End Train\n     - after_cancel_train: [Recorder]\n     - after_train    : [Recorder, ProgressCallback]\n    Start Valid\n       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n      Start Batch Loop\n         - **CBs same as train batch**: []\n      End Batch Loop\n    End Valid\n     - after_cancel_validate: [Recorder]\n     - after_validate : [Recorder, ProgressCallback]\n  End Epoch Loop\n   - after_cancel_epoch: []\n   - after_epoch    : [ShowImgsCallback, Recorder]\nEnd Fit\n - after_cancel_fit: []\n - after_fit      : [ProgressCallback]\n\n\n\ntest_eq(type(learn),Learner)\n\n\nlearn.fit_flat_lin(5,5,2e-4)\n\n\n\n\nepoch\ntrain_loss\nadv_loss_A\nadv_loss_B\nrec_loss_A\nrec_loss_B\nD_A_loss\nD_B_loss\ntime\n\n\n\n\n0\n-1.225994\n-0.965739\n-0.931990\n0.354460\n0.353114\n-0.010898\n-0.002097\n00:06\n\n\n1\n-1.383732\n-0.999190\n-0.998738\n0.258967\n0.261373\n-0.000011\n-0.000268\n00:06\n\n\n2\n-1.448422\n-0.999503\n-0.999257\n0.244793\n0.240065\n0.000066\n-0.000068\n00:06\n\n\n3\n-1.488793\n-0.999653\n-0.999547\n0.240394\n0.223500\n0.000069\n-0.000088\n00:07\n\n\n4\n-1.529549\n-0.999728\n-0.999568\n0.212509\n0.204648\n-0.000021\n-0.000083\n00:07\n\n\n5\n-1.574171\n-0.999754\n-0.999664\n0.190567\n0.178492\n0.000010\n-0.000069\n00:08\n\n\n6\n-1.633542\n-0.999816\n-0.999724\n0.143122\n0.139768\n0.000010\n-0.000058\n00:08\n\n\n7\n-1.675023\n-0.999844\n-0.999737\n0.129500\n0.133948\n0.000027\n-0.000105\n00:08\n\n\n8\n-1.706186\n-0.999842\n-0.999745\n0.118132\n0.129545\n0.000022\n-0.000052\n00:09\n\n\n9\n-1.726011\n-0.999837\n-0.999760\n0.118993\n0.126145\n-0.000012\n-0.000052\n00:09\n\n\n\n\n\n/home/tmabraham/anaconda3/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n  warn(\"Your generator is empty.\")\n\n\n\n\n\n\nlearn.recorder.plot_loss(with_valid=False)"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics for unpaired image-to-image translation",
    "section": "",
    "text": "set_seed(999, reproducible=True)"
  },
  {
    "objectID": "metrics.html#fréchet-inception-distance",
    "href": "metrics.html#fréchet-inception-distance",
    "title": "Metrics for unpaired image-to-image translation",
    "section": "Fréchet Inception Distance",
    "text": "Fréchet Inception Distance\nThis code is based on this implementation and this implementation, adapted to fastai’s metric API.\n\nsource\n\nInceptionV3\n\n InceptionV3 ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFrechetInceptionDistance\n\n FrechetInceptionDistance (model=None, device='cuda', yb_idx=0,\n                           pred_idx=1)\n\nBlueprint for defining a metric\nThe FrechetInceptionDistance metric works by initializing an Inception model, extracting Inception activation features for each batch of predictions and example images (target), and at the end calculate the statistics and the Frechet distance. Below are test for each of these components.\n\nfid = FrechetInceptionDistance(device='cpu')\n\n\nsize = (224, 224, 3)\narrays = [np.zeros(size), np.ones(size) * 0.5, np.ones(size)]*2\nimg_like_tensor = torch.from_numpy(np.array(arrays)).float()\ntest_eq(fid.calc_activations_for_batch(img_like_tensor.permute(0,3,1,2),model=fid.model,device='cpu').shape, (img_like_tensor.shape[0],2048))\n\n\nclass fake_model(nn.Module):\n    def __init__(self): super(fake_model, self).__init__()\n    def forward(self,x): return x.mean(dim=(2,3))\n\nsize = (4, 4, 3)\narrays = [np.zeros(size), np.ones(size) * 0.5, np.ones(size)]\ninput_tensor = torch.from_numpy(np.array(arrays)).float()\n\nstats = fid.calculate_activation_statistics(fid.calc_activations_for_batch(input_tensor.permute(0,3,1,2),model=fake_model()))\ntest_eq(stats[0], np.ones((3,)) * 0.5)\ntest_eq(stats[1], np.ones((3, 3)) * 0.25)\n\n\nm1, m2 = np.zeros((2048,)), np.ones((2048,))\nsigma = np.eye(2048)\n# Given equal covariance, FID is just the squared norm of difference\ntest_eq(fid.calculate_frechet_distance(m1,sigma,m2,sigma), np.sum((m1 - m2)**2))\n\n\nclass FakeLearner():\n    def __init__(self):\n        self.yb = [img_like_tensor.permute(0,3,1,2)]\n        self.pred = [None, img_like_tensor.permute(0,3,1,2)]\nlearn = FakeLearner()\n\n\nfor i in range(5):\n    fid.accumulate(learn)\nprint(fid.value)\n\n-0.0002026209404561996\nCPU times: user 25.9 s, sys: 573 ms, total: 26.5 s\nWall time: 26.4 s"
  },
  {
    "objectID": "metrics.html#quick-test",
    "href": "metrics.html#quick-test",
    "title": "Metrics for unpaired image-to-image translation",
    "section": "Quick Test",
    "text": "Quick Test\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\nfolders = horse2zebra.ls().sorted()\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\ndls = get_dls(trainA_path, trainB_path,num_A=100)\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan,metrics=[FrechetInceptionDistance()],show_img_interval=1)\n\n\n\n\n\nlearn.fit_flat_lin(5,5,2e-4)\n\n\n\n\nepoch\ntrain_loss\nid_loss_A\nid_loss_B\ngen_loss_A\ngen_loss_B\ncyc_loss_A\ncyc_loss_B\nD_A_loss\nD_B_loss\nfrechet_inception_distance\ntime\n\n\n\n\n0\n10.250998\n1.617909\n1.524502\n0.380783\n0.408158\n3.353981\n3.306977\n0.414352\n0.414352\n90.679044\n00:48\n\n\n1\n8.985356\n1.236593\n1.236445\n0.291166\n0.296743\n2.552362\n2.653999\n0.261017\n0.261017\n90.476945\n00:48\n\n\n2\n8.218993\n1.139003\n1.041497\n0.287646\n0.302079\n2.395697\n2.288882\n0.250520\n0.250520\n92.738160\n00:49\n\n\n3\n7.720056\n0.971353\n1.124618\n0.287192\n0.313714\n2.050789\n2.414080\n0.249346\n0.249346\n93.015558\n00:49\n\n\n4\n7.342298\n0.989288\n0.983895\n0.300589\n0.326872\n2.093399\n2.157701\n0.241617\n0.241617\n93.109652\n00:50\n\n\n5\n7.034866\n1.000397\n0.922700\n0.315030\n0.325621\n2.095371\n1.945555\n0.235398\n0.235398\n91.938541\n00:50\n\n\n6\n6.892154\n0.982663\n0.920580\n0.327344\n0.346113\n2.068103\n2.069340\n0.227647\n0.227647\n91.688272\n00:50\n\n\n7\n6.704684\n0.983012\n0.865727\n0.347819\n0.364232\n1.979038\n1.902086\n0.219287\n0.219287\n91.998393\n00:50\n\n\n8\n6.627498\n0.934832\n0.936938\n0.339186\n0.349980\n1.971006\n2.006817\n0.217617\n0.217617\n92.316292\n00:51\n\n\n9\n6.321739\n0.864914\n0.824035\n0.341803\n0.348890\n1.770928\n1.729100\n0.211070\n0.211070\n91.703895\n00:52\n\n\n\n\n\n/home/tmabraham/fastai/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (G_A) that exists in the learner. Use `self.learn.G_A` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/tmabraham/fastai/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (G_B) that exists in the learner. Use `self.learn.G_B` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/tmabraham/fastai/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (D_A) that exists in the learner. Use `self.learn.D_A` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/tmabraham/fastai/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (D_B) that exists in the learner. Use `self.learn.D_B` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n/home/tmabraham/anaconda3/lib/python3.7/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n  warn(\"Your generator is empty.\")"
  },
  {
    "objectID": "models.junyanz.html",
    "href": "models.junyanz.html",
    "title": "Original CycleGAN implementation",
    "section": "",
    "text": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\nWas used before to confirm fastai implementation is correct. Not used elsewhere in the UPIT library.\n\nsource\n\n\n\n PixelDiscriminator (input_nc, ndf=64, norm_layer=&lt;class\n                     'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\nDefines a 1x1 PatchGAN discriminator (pixelGAN)\n\nsource\n\n\n\n\n NLayerDiscriminator (input_nc, ndf=64, n_layers=3, norm_layer=&lt;class\n                      'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\nDefines a PatchGAN discriminator\n\nsource\n\n\n\n\n UnetSkipConnectionBlock (outer_nc, inner_nc, input_nc=None,\n                          submodule=None, outermost=False,\n                          innermost=False, norm_layer=&lt;class\n                          'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                          use_dropout=False)\n\nDefines the Unet submodule with skip connection. X ——————-identity———————- |– downsampling – |submodule| – upsampling –|\n\nsource\n\n\n\n\n UnetGenerator (input_nc, output_nc, num_downs, ngf=64, norm_layer=&lt;class\n                'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                use_dropout=False)\n\nCreate a Unet-based generator\n\nsource\n\n\n\n\n ResnetBlock (dim, padding_type, norm_layer, use_dropout, use_bias)\n\nDefine a Resnet block\n\nsource\n\n\n\n\n ResnetGenerator (input_nc, output_nc, ngf=64, norm_layer=&lt;class\n                  'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                  use_dropout=False, n_blocks=6, padding_type='reflect')\n\nResnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\nWe adapt Torch code and idea from Justin Johnson’s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n\nsource\n\n\n\n\n cal_gradient_penalty (netD, real_data, fake_data, device, type='mixed',\n                       constant=1.0, lambda_gp=10.0)\n\nCalculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\nArguments: netD (network) – discriminator network real_data (tensor array) – real images fake_data (tensor array) – generated images from the generator device (str) – GPU / CPU: from torch.device(‘cuda:{}’.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(‘cpu’) type (str) – if we mix real and fake data or not [real | fake | mixed]. constant (float) – the constant used in formula ( | |gradient||_2 - constant)^2 lambda_gp (float) – weight for this loss\nReturns the gradient penalty loss\n\nsource\n\n\n\n\n GANLoss (gan_mode, target_real_label=1.0, target_fake_label=0.0)\n\nDefine different GAN objectives.\nThe GANLoss class abstracts away the need to create the target label tensor that has the same size as the input.\n\nsource\n\n\n\n\n define_D (input_nc, ndf, netD, n_layers_D=3, norm='batch',\n           init_type='normal', init_gain=0.02, gpu_ids=[])\n\nCreate a discriminator\nParameters: input_nc (int) – the number of channels in input images ndf (int) – the number of filters in the first conv layer netD (str) – the architecture’s name: basic | n_layers | pixel n_layers_D (int) – the number of conv layers in the discriminator; effective when netD==‘n_layers’ norm (str) – the type of normalization layers used in the network. init_type (str) – the name of the initialization method. init_gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturns a discriminator\nOur current implementation provides three types of discriminators: [basic]: ‘PatchGAN’ classifier described in the original pix2pix paper. It can classify whether 70×70 overlapping patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion.\n[n_layers]: With this mode, you can specify the number of conv layers in the discriminator\nwith the parameter &lt;n_layers_D&gt; (default=3 as used in [basic] (PatchGAN).)\n\n[pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\nIt encourages greater color diversity but has no effect on spatial statistics.\nThe discriminator has been initialized by . It uses Leakly RELU for non-linearity.\n\nsource\n\n\n\n\n define_G (input_nc, output_nc, ngf, netG, norm='batch',\n           use_dropout=False, init_type='normal', init_gain=0.02,\n           gpu_ids=[])\n\nCreate a generator\nParameters: input_nc (int) – the number of channels in input images output_nc (int) – the number of channels in output images ngf (int) – the number of filters in the last conv layer netG (str) – the architecture’s name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) – the name of normalization layers used in the network: batch | instance | none use_dropout (bool) – if use dropout layers. init_type (str) – the name of our initialization method. init_gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturns a generator\nOur current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597\nResnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\nResnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\nWe adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\nThe generator has been initialized by . It uses RELU for non-linearity.\n\nsource\n\n\n\n\n init_net (net, init_type='normal', init_gain=0.02, gpu_ids=[])\n\nInitialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) – the network to be initialized init_type (str) – the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturn an initialized network.\n\nsource\n\n\n\n\n init_weights (net, init_type='normal', init_gain=0.02)\n\nInitialize network weights.\nParameters: net (network) – network to be initialized init_type (str) – the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) – scaling factor for normal, xavier and orthogonal.\nWe use ‘normal’ in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself.\n\nsource\n\n\n\n\n get_scheduler (optimizer, opt)\n\nReturn a learning rate scheduler\nParameters: optimizer – the optimizer of the network opt (option class) – stores all the experiment flags; needs to be a subclass of BaseOptions．　 opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\nFor ‘linear’, we keep the same learning rate for the first &lt;opt.n_epochs&gt; epochs and linearly decay the rate to zero over the next &lt;opt.n_epochs_decay&gt; epochs. For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers. See https://pytorch.org/docs/stable/optim.html for more details.\n\nsource\n\n\n\n\n get_norm_layer (norm_type='instance')\n\nReturn a normalization layer\nParameters: norm_type (str) – the name of the normalization layer: batch | instance | none\nFor BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n\nsource\n\n\n\n\n Identity (*args, **kwargs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "models.junyanz.html#taken-from-here",
    "href": "models.junyanz.html#taken-from-here",
    "title": "Original CycleGAN implementation",
    "section": "",
    "text": "https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\nWas used before to confirm fastai implementation is correct. Not used elsewhere in the UPIT library.\n\nsource\n\n\n\n PixelDiscriminator (input_nc, ndf=64, norm_layer=&lt;class\n                     'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\nDefines a 1x1 PatchGAN discriminator (pixelGAN)\n\nsource\n\n\n\n\n NLayerDiscriminator (input_nc, ndf=64, n_layers=3, norm_layer=&lt;class\n                      'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\nDefines a PatchGAN discriminator\n\nsource\n\n\n\n\n UnetSkipConnectionBlock (outer_nc, inner_nc, input_nc=None,\n                          submodule=None, outermost=False,\n                          innermost=False, norm_layer=&lt;class\n                          'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                          use_dropout=False)\n\nDefines the Unet submodule with skip connection. X ——————-identity———————- |– downsampling – |submodule| – upsampling –|\n\nsource\n\n\n\n\n UnetGenerator (input_nc, output_nc, num_downs, ngf=64, norm_layer=&lt;class\n                'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                use_dropout=False)\n\nCreate a Unet-based generator\n\nsource\n\n\n\n\n ResnetBlock (dim, padding_type, norm_layer, use_dropout, use_bias)\n\nDefine a Resnet block\n\nsource\n\n\n\n\n ResnetGenerator (input_nc, output_nc, ngf=64, norm_layer=&lt;class\n                  'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n                  use_dropout=False, n_blocks=6, padding_type='reflect')\n\nResnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\nWe adapt Torch code and idea from Justin Johnson’s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n\nsource\n\n\n\n\n cal_gradient_penalty (netD, real_data, fake_data, device, type='mixed',\n                       constant=1.0, lambda_gp=10.0)\n\nCalculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\nArguments: netD (network) – discriminator network real_data (tensor array) – real images fake_data (tensor array) – generated images from the generator device (str) – GPU / CPU: from torch.device(‘cuda:{}’.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(‘cpu’) type (str) – if we mix real and fake data or not [real | fake | mixed]. constant (float) – the constant used in formula ( | |gradient||_2 - constant)^2 lambda_gp (float) – weight for this loss\nReturns the gradient penalty loss\n\nsource\n\n\n\n\n GANLoss (gan_mode, target_real_label=1.0, target_fake_label=0.0)\n\nDefine different GAN objectives.\nThe GANLoss class abstracts away the need to create the target label tensor that has the same size as the input.\n\nsource\n\n\n\n\n define_D (input_nc, ndf, netD, n_layers_D=3, norm='batch',\n           init_type='normal', init_gain=0.02, gpu_ids=[])\n\nCreate a discriminator\nParameters: input_nc (int) – the number of channels in input images ndf (int) – the number of filters in the first conv layer netD (str) – the architecture’s name: basic | n_layers | pixel n_layers_D (int) – the number of conv layers in the discriminator; effective when netD==‘n_layers’ norm (str) – the type of normalization layers used in the network. init_type (str) – the name of the initialization method. init_gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturns a discriminator\nOur current implementation provides three types of discriminators: [basic]: ‘PatchGAN’ classifier described in the original pix2pix paper. It can classify whether 70×70 overlapping patches are real or fake. Such a patch-level discriminator architecture has fewer parameters than a full-image discriminator and can work on arbitrarily-sized images in a fully convolutional fashion.\n[n_layers]: With this mode, you can specify the number of conv layers in the discriminator\nwith the parameter &lt;n_layers_D&gt; (default=3 as used in [basic] (PatchGAN).)\n\n[pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\nIt encourages greater color diversity but has no effect on spatial statistics.\nThe discriminator has been initialized by . It uses Leakly RELU for non-linearity.\n\nsource\n\n\n\n\n define_G (input_nc, output_nc, ngf, netG, norm='batch',\n           use_dropout=False, init_type='normal', init_gain=0.02,\n           gpu_ids=[])\n\nCreate a generator\nParameters: input_nc (int) – the number of channels in input images output_nc (int) – the number of channels in output images ngf (int) – the number of filters in the last conv layer netG (str) – the architecture’s name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128 norm (str) – the name of normalization layers used in the network: batch | instance | none use_dropout (bool) – if use dropout layers. init_type (str) – the name of our initialization method. init_gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturns a generator\nOur current implementation provides two types of generators: U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images) The original U-Net paper: https://arxiv.org/abs/1505.04597\nResnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\nResnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\nWe adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\nThe generator has been initialized by . It uses RELU for non-linearity.\n\nsource\n\n\n\n\n init_net (net, init_type='normal', init_gain=0.02, gpu_ids=[])\n\nInitialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights Parameters: net (network) – the network to be initialized init_type (str) – the name of an initialization method: normal | xavier | kaiming | orthogonal gain (float) – scaling factor for normal, xavier and orthogonal. gpu_ids (int list) – which GPUs the network runs on: e.g., 0,1,2\nReturn an initialized network.\n\nsource\n\n\n\n\n init_weights (net, init_type='normal', init_gain=0.02)\n\nInitialize network weights.\nParameters: net (network) – network to be initialized init_type (str) – the name of an initialization method: normal | xavier | kaiming | orthogonal init_gain (float) – scaling factor for normal, xavier and orthogonal.\nWe use ‘normal’ in the original pix2pix and CycleGAN paper. But xavier and kaiming might work better for some applications. Feel free to try yourself.\n\nsource\n\n\n\n\n get_scheduler (optimizer, opt)\n\nReturn a learning rate scheduler\nParameters: optimizer – the optimizer of the network opt (option class) – stores all the experiment flags; needs to be a subclass of BaseOptions．　 opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\nFor ‘linear’, we keep the same learning rate for the first &lt;opt.n_epochs&gt; epochs and linearly decay the rate to zero over the next &lt;opt.n_epochs_decay&gt; epochs. For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers. See https://pytorch.org/docs/stable/optim.html for more details.\n\nsource\n\n\n\n\n get_norm_layer (norm_type='instance')\n\nReturn a normalization layer\nParameters: norm_type (str) – the name of the normalization layer: batch | instance | none\nFor BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev). For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n\nsource\n\n\n\n\n Identity (*args, **kwargs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "models.dualgan.html",
    "href": "models.dualgan.html",
    "title": "DualGAN model",
    "section": "",
    "text": "We use the models that were introduced in the DualGAN paper. The original implementation is here."
  },
  {
    "objectID": "models.dualgan.html#generator",
    "href": "models.dualgan.html#generator",
    "title": "DualGAN model",
    "section": "Generator",
    "text": "Generator\n\nsource\n\nweights_init_normal\n\n weights_init_normal (m)\n\n\nsource\n\n\nUNetUp\n\n UNetUp (in_size, out_size, dropout=0.0)\n\nExpanding layers of the Unet used in DualGAN\n\nsource\n\n\nUNetDown\n\n UNetDown (in_size, out_size, normalize=True, dropout=0.0)\n\nContracting layers of the Unet used in DualGAN\n\nsource\n\n\nDualGANGenerator\n\n DualGANGenerator (channels=3)\n\nGenerator model for the DualGAN\n\n\nTest generator\nLet’s test for a few things: 1. The generator can indeed be initialized correctly 2. A random image can be passed into the model successfully with the correct size output\nFirst let’s create a random batch:\n\nimg1 = torch.randn(4,3,256,256)\n\n\nm = DualGANGenerator(3)\nwith torch.no_grad():\n    out1 = m(img1)\nout1.shape\n\ntorch.Size([4, 3, 256, 256])\n\n\n\ntest_eq(out1.shape, torch.Size([4, 3, 256, 256]))"
  },
  {
    "objectID": "models.dualgan.html#discriminator",
    "href": "models.dualgan.html#discriminator",
    "title": "DualGAN model",
    "section": "Discriminator",
    "text": "Discriminator\nAs described in the DualGAN paper, we will use a 70x70 PatchGAN, the same discriminator for the CycleGAN.\n\nD = discriminator(3)\nprint(D)\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n  (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n  (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n  (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n  (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n  (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n)\n\n\n\nsource\n\nDualGAN\n\n DualGAN (ch_in:int=3, n_features:int=64, disc_layers:int=3,\n          lsgan:bool=False, drop:float=0.0,\n          norm_layer:torch.nn.modules.module.Module=None)\n\nDualGAN model.\nWhen called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators).\nAttributes:\nG_A (nn.Module): takes real input B and generates fake input A\nG_B (nn.Module): takes real input A and generates fake input B\nD_A (nn.Module): trained to make the difference between real input A and fake input A\nD_B (nn.Module): trained to make the difference between real input B and fake input B\n\nsource\n\n\nDualGAN.__init__\n\n DualGAN.__init__ (ch_in:int=3, n_features:int=64, disc_layers:int=3,\n                   lsgan:bool=False, drop:float=0.0,\n                   norm_layer:torch.nn.modules.module.Module=None)\n\nConstructor for DualGAN model.\nArguments:\nch_in (int): Number of input channels (default=3)\nn_features (int): Number of input features (default=64)\ndisc_layers (int): Number of discriminator layers (default=3)\nlsgan (bool): LSGAN training objective (output unnormalized float) or not? (default=True)\nnorm_layer (nn.Module): Type of normalization layer to use in the models (default=None)\n\nsource\n\n\nDualGAN.forward\n\n DualGAN.forward (input)\n\nForward function for DualGAN model. The input is a tuple of a batch of real images from both domains A and B.\n\n\nQuick model tests\nAgain, let’s check that the model can be called sucsessfully and outputs the correct shapes.\n\ndualgan_model = DualGAN()\nimg1 = torch.randn(4,3,256,256)\nimg2 = torch.randn(4,3,256,256)\n\n\nwith torch.no_grad(): dualgan_output = dualgan_model((img1,img2))\n\nCPU times: user 18.9 s, sys: 1.57 s, total: 20.5 s\nWall time: 447 ms\n\n\n\ntest_eq(len(dualgan_output),4)\nfor output_batch in dualgan_output:\n    test_eq(output_batch.shape,img1.shape)\n\n\ndualgan_model.push_to_hub('upit-dualgan-test')\n\nCloning https://huggingface.co/tmabraham/upit-dualgan-test into local empty directory.\nTo https://huggingface.co/tmabraham/upit-dualgan-test\n   dccaa0f..f8d92db  main -&gt; main\n\n\n\n\n\n\n'https://huggingface.co/tmabraham/upit-dualgan-test/commit/f8d92db7854429ca64335e9ab698d7e7f2f44feb'\n\n\n\ndualgan_model.from_pretrained('tmabraham/upit-dualgan-test')"
  },
  {
    "objectID": "train.cyclegan.html",
    "href": "train.cyclegan.html",
    "title": "CycleGAN training loop",
    "section": "",
    "text": "set_seed(999, reproducible=True)"
  },
  {
    "objectID": "train.cyclegan.html#the-loss-function",
    "href": "train.cyclegan.html#the-loss-function",
    "title": "CycleGAN training loop",
    "section": "The Loss Function",
    "text": "The Loss Function\nLet’s start out by writing the loss function for the CycleGAN model. The main loss used to train the generators. It has three parts: - the classic GAN loss: they must make the discriminator believe their images are real. - identity loss: if they are given an image from the domain they are trying to imitate, they should return the same thing - cycle loss: if an image from domain A goes through the generator that imitates domain B then through the generator that imitates domain A, it should be reconstructed as the same initial image. Same for domain B and switching the generators\n\nsource\n\nCycleGANLoss\n\n CycleGANLoss (cgan:torch.nn.modules.module.Module, l_A:float=10.0,\n               l_B:float=10, l_idt:float=0.5, lsgan:bool=True)\n\nCycleGAN loss function. The individual loss terms are also atrributes of this class that are accessed by fastai for recording during training.\nAttributes:\nself.cgan (nn.Module): The CycleGAN model.\nself.l_A (float): lambda_A, weight of domain A losses.\nself.l_B (float): lambda_B, weight of domain B losses.\nself.l_idt (float): lambda_idt, weight of identity lossees.\nself.crit (AdaptiveLoss): The adversarial loss function (either a BCE or MSE loss depending on lsgan argument)\nself.real_A and self.real_B (fastai.torch_core.TensorImage): Real images from domain A and B.\nself.id_loss_A (torch.FloatTensor): The identity loss for domain A calculated in the forward function\nself.id_loss_B (torch.FloatTensor): The identity loss for domain B calculated in the forward function\nself.gen_loss (torch.FloatTensor): The generator loss calculated in the forward function\nself.cyc_loss (torch.FloatTensor): The cyclic loss calculated in the forward function\n\nsource\n\n\nCycleGANLoss.__init__\n\n CycleGANLoss.__init__ (cgan:torch.nn.modules.module.Module,\n                        l_A:float=10.0, l_B:float=10, l_idt:float=0.5,\n                        lsgan:bool=True)\n\nConstructor for CycleGAN loss.\nArguments:\ncgan (nn.Module): The CycleGAN model.\nl_A (float): weight of domain A losses. (default=10)\nl_B (float): weight of domain B losses. (default=10)\nl_idt (float): weight of identity losses. (default=0.5)\nlsgan (bool): Whether or not to use LSGAN objective. (default=True)\n\nsource\n\n\nCycleGANLoss.set_input\n\n CycleGANLoss.set_input (input)\n\nset self.real_A and self.real_B for future loss calculation\n\nsource\n\n\nCycleGANLoss.forward\n\n CycleGANLoss.forward (output, target)\n\nForward function of the CycleGAN loss function. The generated images are passed in as output (which comes from the model) and the generator loss is returned."
  },
  {
    "objectID": "train.cyclegan.html#training-loop-callback",
    "href": "train.cyclegan.html#training-loop-callback",
    "title": "CycleGAN training loop",
    "section": "Training loop callback",
    "text": "Training loop callback\nLet’s now write the main callback to train a CycleGAN model.\nFastai’s callback system is very flexible, allowing us to adjust the traditional training loop in any conceivable way possible. Let’s use it for GAN training.\nWe have the _set_trainable function that is called with arguments telling which networks need to be put in training mode or which need to be frozen.\nWhen we start training before_train, we define separate optimizers. self.opt_G for the generators and self.opt_D for the discriminators. Then we put the generators in training mode (with _set_trainable).\nBefore passing the batch into the model (before_batch), we have to fix it since the domain B image was kept as the target, but it also needs to be passed into the model. We also set the inputs for the loss function.\nIn after_batch, we calculate the discriminator losses, backpropagate, and update the weights of both the discriminators. The main training loop will train the generators.\n\nsource\n\nCycleGANTrainer\n\n CycleGANTrainer ()\n\nLearner Callback for training a CycleGAN model.\n\nsource\n\n\nCycleGANTrainer._set_trainable\n\n CycleGANTrainer._set_trainable (disc=False)\n\nPut the generators or discriminators in training mode depending on arguments.\n\nsource\n\n\nCycleGANTrainer.after_batch\n\n CycleGANTrainer.after_batch (**kwargs)\n\nDiscriminator training loop\n\nsource\n\n\nShowImgsCallback\n\n ShowImgsCallback (imgA:bool=False, imgB:bool=True,\n                   show_img_interval:int=10)\n\nUpdate the progress bar with input and prediction images\n\nsource\n\n\nShowImgsCallback.__init__\n\n ShowImgsCallback.__init__ (imgA:bool=False, imgB:bool=True,\n                            show_img_interval:int=10)\n\nIf imgA is True, display B-to-A conversion example during training. If imgB is True, display A-to-B conversion example. Show images every show_img_interval epochs.\n\nsource\n\n\nShowImgsCallback.after_epoch\n\n ShowImgsCallback.after_epoch ()\n\nUpdate images"
  },
  {
    "objectID": "train.cyclegan.html#cyclegan-lr-scheduler",
    "href": "train.cyclegan.html#cyclegan-lr-scheduler",
    "title": "CycleGAN training loop",
    "section": "CycleGAN LR scheduler",
    "text": "CycleGAN LR scheduler\nThe original CycleGAN paper started with a period of constant learning rate and a period of linearly decaying learning rate. Let’s make a scheduler to implement this (with other possibilities as well). Fastai already comes with many types of hyperparameter schedules, and new ones can be created by combining existing ones. Let’s see how to do this:\n\nsource\n\ncombined_flat_anneal\n\n combined_flat_anneal (pct:float, start_lr:float, end_lr:float=0,\n                       curve_type:str='linear')\n\nCreate a schedule with constant learning rate start_lr for pct proportion of the training, and a curve_type learning rate (till end_lr) for remaining portion of training.\nArguments: pct (float): Proportion of training with a constant learning rate.\nstart_lr (float): Desired starting learning rate, used for beginnning pct of training.\nend_lr (float): Desired end learning rate, training will conclude at this learning rate.\ncurve_type (str): Curve type for learning rate annealing. Options are ‘linear’, ‘cosine’, and ‘exponential’.\n\np = torch.linspace(0.,1,200)\nplt.plot(p, [combined_flat_anneal(0.5,1,1e-2,curve_type='linear')(o) for o in p],label = 'linear annealing')\nplt.plot(p, [combined_flat_anneal(0.5,1,1e-2,curve_type='cosine')(o) for o in p],label = 'cosine annealing')\nplt.plot(p, [combined_flat_anneal(0.5,1,1e-2,curve_type='exponential')(o) for o in p],label = 'exponential annealing')\nplt.legend()\nplt.title('Constant+annealing LR schedules')\n\nText(0.5, 1.0, 'Constant+annealing LR schedules')\n\n\n\n\n\nNow that we have the learning rate schedule, we can write a quick training function that can be added as a method to Learner using @patch decorator. Function is inspired by this code.\n\nsource\n\n\nLearner.fit_flat_lin\n\n Learner.fit_flat_lin (n_epochs:int=100, n_epochs_decay:int=100,\n                       start_lr:float=None, end_lr:float=0,\n                       curve_type:str='linear', wd:float=None, cbs=None,\n                       reset_opt=False)\n\nFit self.model for n_epoch at flat start_lr before curve_type annealing to end_lr with weight decay of wd and callbacks cbs.\n\nfrom fastai.test_utils import *\n\n\nlearn = synth_learner()\nlearn.fit_flat_lin(n_epochs=2,n_epochs_decay=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n23.626183\n20.342035\n00:00\n\n\n1\n20.433842\n13.960705\n00:00\n\n\n2\n17.039629\n9.767084\n00:00\n\n\n3\n14.475015\n8.587482\n00:00\n\n\n\n\n\n\nlearn.recorder.plot_sched()"
  },
  {
    "objectID": "train.cyclegan.html#cyclegan-learner-construction",
    "href": "train.cyclegan.html#cyclegan-learner-construction",
    "title": "CycleGAN training loop",
    "section": "CycleGAN Learner construction",
    "text": "CycleGAN Learner construction\nBelow, we now define a method for initializing a Learner with the CycleGAN model and training callback.\n\nsource\n\ncycle_learner\n\n cycle_learner (dls:fastai.data.load.DataLoader,\n                m:upit.models.cyclegan.CycleGAN, opt_func=&lt;function Adam&gt;,\n                loss_func=&lt;class '__main__.CycleGANLoss'&gt;,\n                show_imgs:bool=True, imgA:bool=True, imgB:bool=True,\n                show_img_interval:bool=10, metrics:list=[], cbs:list=[],\n                lr:Union[float,slice]=0.001, splitter:&lt;built-\n                infunctioncallable&gt;=&lt;function trainable_params&gt;,\n                path:Union[str,pathlib.Path,NoneType]=None,\n                model_dir:Union[str,pathlib.Path]='models',\n                wd:Union[float,int,NoneType]=None, wd_bn_bias:bool=False,\n                train_bn:bool=True, moms:tuple=(0.95, 0.85, 0.95),\n                default_cbs:bool=True)\n\nInitialize and return a Learner object with the data in dls, CycleGAN model m, optimizer function opt_func, metrics metrics, and callbacks cbs. Additionally, if show_imgs is True, it will show intermediate predictions during training. It will show domain B-to-A predictions if imgA is True and/or domain A-to-B predictions if imgB is True. Additionally, it will show images every show_img_interval epochs. OtherLearner` arguments can be passed as well.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndls\nDataLoaders\n\nDataLoaders containing fastai or PyTorch DataLoaders\n\n\nm\nCycleGAN\n\n\n\n\nopt_func\nOptimizer | OptimWrapper\nAdam\nOptimization function for training\n\n\nloss_func\ncallable | None\nNone\nLoss function. Defaults to dls loss\n\n\nshow_imgs\nbool\nTrue\n\n\n\nimgA\nbool\nTrue\n\n\n\nimgB\nbool\nTrue\n\n\n\nshow_img_interval\nbool\n10\n\n\n\nmetrics\ncallable | MutableSequence | None\nNone\nMetrics to calculate on validation set\n\n\ncbs\nCallback | MutableSequence | None\nNone\nCallbacks to add to Learner\n\n\nlr\nfloat | slice\n0.001\nDefault learning rate\n\n\nsplitter\ncallable\ntrainable_params\nSplit model into parameter groups. Defaults to one parameter group\n\n\npath\nstr | Path | None\nNone\nParent directory to save, load, and export models. Defaults to dls path\n\n\nmodel_dir\nstr | Path\nmodels\nSubdirectory to save and load models\n\n\nwd\nfloat | int | None\nNone\nDefault weight decay\n\n\nwd_bn_bias\nbool\nFalse\nApply weight decay to normalization and bias parameters\n\n\ntrain_bn\nbool\nTrue\nTrain frozen normalization layers\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nDefault momentum for schedulers\n\n\ndefault_cbs\nbool\nTrue\nInclude default Callbacks"
  },
  {
    "objectID": "train.cyclegan.html#quick-test",
    "href": "train.cyclegan.html#quick-test",
    "title": "CycleGAN training loop",
    "section": "Quick Test",
    "text": "Quick Test\n\nhorse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n\n\nfolders = horse2zebra.ls().sorted()\n\n\ntrainA_path = folders[2]\ntrainB_path = folders[3]\ntestA_path = folders[0]\ntestB_path = folders[1]\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=100)\n\n\ncycle_gan = CycleGAN(3,3,64)\nlearn = cycle_learner(dls, cycle_gan,show_img_interval=1)\n\n\nlearn.show_training_loop()\n\nStart Fit\n   - before_fit     : [TrainEvalCallback, ShowImgsCallback, Recorder, ProgressCallback]\n  Start Epoch Loop\n     - before_epoch   : [Recorder, ProgressCallback]\n    Start Train\n       - before_train   : [TrainEvalCallback, CycleGANTrainer, Recorder, ProgressCallback]\n      Start Batch Loop\n         - before_batch   : [CycleGANTrainer]\n         - after_pred     : []\n         - after_loss     : []\n         - before_backward: []\n         - before_step    : []\n         - after_step     : [CycleGANTrainer]\n         - after_cancel_batch: []\n         - after_batch    : [TrainEvalCallback, CycleGANTrainer, Recorder, ProgressCallback]\n      End Batch Loop\n    End Train\n     - after_cancel_train: [Recorder]\n     - after_train    : [Recorder, ProgressCallback]\n    Start Valid\n       - before_validate: [TrainEvalCallback, CycleGANTrainer, Recorder, ProgressCallback]\n      Start Batch Loop\n         - **CBs same as train batch**: []\n      End Batch Loop\n    End Valid\n     - after_cancel_validate: [Recorder]\n     - after_validate : [Recorder, ProgressCallback]\n  End Epoch Loop\n   - after_cancel_epoch: []\n   - after_epoch    : [ShowImgsCallback, Recorder]\nEnd Fit\n - after_cancel_fit: []\n - after_fit      : [ProgressCallback]\n\n\n\ntest_eq(type(learn),Learner)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\n/home/tmabraham/anaconda3/envs/UPIT/lib/python3.9/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n  warn(\"Your generator is empty.\")\n/home/tmabraham/anaconda3/envs/UPIT/lib/python3.9/site-packages/fastai/learner.py:55: UserWarning: Could not load the optimizer state.\n  if with_opt: warn(\"Could not load the optimizer state.\")\n\n\nSuggestedLRs(valley=7.585775892948732e-05)\n\n\n\n\n\n\nlearn.fit_flat_lin(5,5,2e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nid_loss_A\nid_loss_B\ngen_loss_A\ngen_loss_B\ncyc_loss_A\ncyc_loss_B\nD_A_loss\nD_B_loss\ntime\n\n\n\n\n0\n11.072145\n1.640516\n1.656911\n0.375354\n0.837723\n3.378847\n3.488000\n0.335865\n0.487188\n00:06\n\n\n1\n9.442083\n1.247157\n1.282357\n0.277883\n0.259882\n2.677947\n2.745830\n0.266643\n0.268916\n00:07\n\n\n2\n8.649205\n1.111012\n1.206819\n0.303226\n0.315038\n2.388910\n2.627970\n0.253195\n0.241816\n00:07\n\n\n3\n8.046769\n1.097790\n1.035227\n0.293115\n0.301284\n2.343968\n2.258730\n0.247176\n0.234314\n00:07\n\n\n4\n7.690011\n1.046379\n1.057508\n0.290288\n0.330028\n2.205441\n2.325534\n0.250845\n0.219718\n00:07\n\n\n5\n7.262819\n1.013530\n0.901757\n0.309617\n0.314876\n2.126933\n2.008702\n0.240130\n0.225835\n00:08\n\n\n6\n6.940499\n0.943555\n0.880684\n0.301979\n0.329805\n2.035716\n1.983523\n0.242219\n0.223504\n00:08\n\n\n7\n6.726294\n0.928122\n0.899857\n0.307265\n0.320807\n1.949748\n2.006305\n0.231187\n0.210763\n00:08\n\n\n8\n6.461397\n0.884317\n0.830321\n0.305884\n0.326825\n1.902252\n1.801770\n0.229438\n0.213273\n00:08\n\n\n9\n6.317296\n0.896565\n0.852179\n0.321770\n0.320993\n1.873770\n1.843444\n0.221933\n0.220043\n00:08\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss(with_valid=False)\n\n\n\n\n\ndls = get_dls(trainA_path, trainB_path,num_A=100)\n\n\ncycle_gan = CycleGAN(3,3,64)\nloss_func = partial(CycleGANLoss, l_idt=0)\nlearn = cycle_learner(dls, cycle_gan, loss_func=loss_func, show_img_interval=1)\n\n\ntest_eq(type(learn),Learner)\n\n\ntest_eq(learn.loss_func.l_idt, 0)\n\n\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=5.248074739938602e-05)\n\n\n\n\n\n\nlearn.fit_flat_lin(5,5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nid_loss_A\nid_loss_B\ngen_loss_A\ngen_loss_B\ncyc_loss_A\ncyc_loss_B\nD_A_loss\nD_B_loss\ntime\n\n\n\n\n0\n8.013542\n0.000000\n0.000000\n0.415603\n0.605523\n3.475492\n3.725508\n0.511603\n0.547689\n00:06\n\n\n1\n7.033264\n0.000000\n0.000000\n0.286641\n0.327472\n2.772444\n3.095340\n0.276890\n0.271155\n00:07\n\n\n2\n6.641585\n0.000000\n0.000000\n0.284850\n0.292105\n2.692654\n2.993103\n0.252079\n0.252860\n00:07\n\n\n3\n6.357308\n0.000000\n0.000000\n0.275745\n0.292774\n2.705673\n2.779573\n0.255172\n0.255423\n00:07\n\n\n4\n6.097081\n0.000000\n0.000000\n0.283673\n0.298791\n2.574903\n2.593902\n0.241834\n0.228963\n00:07\n\n\n5\n5.914270\n0.000000\n0.000000\n0.284902\n0.327440\n2.468222\n2.578838\n0.237878\n0.232171\n00:08\n\n\n6\n5.796364\n0.000000\n0.000000\n0.307585\n0.299103\n2.421084\n2.611041\n0.230458\n0.239380\n00:08\n\n\n7\n5.586002\n0.000000\n0.000000\n0.320189\n0.331698\n2.315644\n2.325492\n0.228474\n0.232233\n00:08\n\n\n8\n5.458314\n0.000000\n0.000000\n0.331234\n0.284459\n2.333813\n2.349262\n0.218699\n0.244563\n00:08\n\n\n9\n5.266456\n0.000000\n0.000000\n0.287622\n0.288966\n2.267640\n2.139976\n0.230699\n0.244301\n00:08\n\n\n\n\n\n\n\n\n\nlearn.recorder.plot_loss(with_valid=False)"
  }
]