---

title: `01b_models_junyanz.ipynb`

keywords: fastai
sidebar: home_sidebar



---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01b_models_junyanz.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Taken-from-here:">Taken from here:<a class="anchor-link" href="#Taken-from-here:"> </a></h2><p><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py</a></p>
<p>Was used before to confirm fastai implementation is correct. Not used elsewhere in the <code>muse2he</code> library.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Identity" class="doc_header"><code>class</code> <code>Identity</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Identity</code>() :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_norm_layer" class="doc_header"><code>get_norm_layer</code><a href="__main__.py#L19" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_norm_layer</code>(<strong><code>norm_type</code></strong>=<em><code>'instance'</code></em>)</p>
</blockquote>
<p>Return a normalization layer</p>
<p>Parameters:
    norm_type (str) -- the name of the normalization layer: batch | instance | none</p>
<p>For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).
For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_scheduler" class="doc_header"><code>get_scheduler</code><a href="__main__.py#L39" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_scheduler</code>(<strong><code>optimizer</code></strong>, <strong><code>opt</code></strong>)</p>
</blockquote>
<p>Return a learning rate scheduler</p>
<p>Parameters:
    optimizer          -- the optimizer of the network
    opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　
                          opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine</p>
<p>For 'linear', we keep the same learning rate for the first &lt;opt.n_epochs&gt; epochs
and linearly decay the rate to zero over the next &lt;opt.n_epochs_decay&gt; epochs.
For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.
See <a href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a> for more details.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="init_weights" class="doc_header"><code>init_weights</code><a href="__main__.py#L68" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>init_weights</code>(<strong><code>net</code></strong>, <strong><code>init_type</code></strong>=<em><code>'normal'</code></em>, <strong><code>init_gain</code></strong>=<em><code>0.02</code></em>)</p>
</blockquote>
<p>Initialize network weights.</p>
<p>Parameters:
    net (network)   -- network to be initialized
    init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal
    init_gain (float)    -- scaling factor for normal, xavier and orthogonal.</p>
<p>We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might
work better for some applications. Feel free to try yourself.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="init_net" class="doc_header"><code>init_net</code><a href="__main__.py#L102" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>init_net</code>(<strong><code>net</code></strong>, <strong><code>init_type</code></strong>=<em><code>'normal'</code></em>, <strong><code>init_gain</code></strong>=<em><code>0.02</code></em>, <strong><code>gpu_ids</code></strong>=<em><code>[]</code></em>)</p>
</blockquote>
<p>Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights
Parameters:
    net (network)      -- the network to be initialized
    init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal
    gain (float)       -- scaling factor for normal, xavier and orthogonal.
    gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2</p>
<p>Return an initialized network.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="define_G" class="doc_header"><code>define_G</code><a href="__main__.py#L120" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>define_G</code>(<strong><code>input_nc</code></strong>, <strong><code>output_nc</code></strong>, <strong><code>ngf</code></strong>, <strong><code>netG</code></strong>, <strong><code>norm</code></strong>=<em><code>'batch'</code></em>, <strong><code>use_dropout</code></strong>=<em><code>False</code></em>, <strong><code>init_type</code></strong>=<em><code>'normal'</code></em>, <strong><code>init_gain</code></strong>=<em><code>0.02</code></em>, <strong><code>gpu_ids</code></strong>=<em><code>[]</code></em>)</p>
</blockquote>
<p>Create a generator</p>
<p>Parameters:
    input_nc (int) -- the number of channels in input images
    output_nc (int) -- the number of channels in output images
    ngf (int) -- the number of filters in the last conv layer
    netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128
    norm (str) -- the name of normalization layers used in the network: batch | instance | none
    use_dropout (bool) -- if use dropout layers.
    init_type (str)    -- the name of our initialization method.
    init_gain (float)  -- scaling factor for normal, xavier and orthogonal.
    gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2</p>
<p>Returns a generator</p>
<p>Our current implementation provides two types of generators:
    U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)
    The original U-Net paper: <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></p>

<pre><code>Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)
Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.
We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).


</code></pre>
<p>The generator has been initialized by <init_net>. It uses RELU for non-linearity.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="define_D" class="doc_header"><code>define_D</code><a href="__main__.py#L163" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>define_D</code>(<strong><code>input_nc</code></strong>, <strong><code>ndf</code></strong>, <strong><code>netD</code></strong>, <strong><code>n_layers_D</code></strong>=<em><code>3</code></em>, <strong><code>norm</code></strong>=<em><code>'batch'</code></em>, <strong><code>init_type</code></strong>=<em><code>'normal'</code></em>, <strong><code>init_gain</code></strong>=<em><code>0.02</code></em>, <strong><code>gpu_ids</code></strong>=<em><code>[]</code></em>)</p>
</blockquote>
<p>Create a discriminator</p>
<p>Parameters:
    input_nc (int)     -- the number of channels in input images
    ndf (int)          -- the number of filters in the first conv layer
    netD (str)         -- the architecture's name: basic | n_layers | pixel
    n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'
    norm (str)         -- the type of normalization layers used in the network.
    init_type (str)    -- the name of the initialization method.
    init_gain (float)  -- scaling factor for normal, xavier and orthogonal.
    gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2</p>
<p>Returns a discriminator</p>
<p>Our current implementation provides three types of discriminators:
    [basic]: 'PatchGAN' classifier described in the original pix2pix paper.
    It can classify whether 70×70 overlapping patches are real or fake.
    Such a patch-level discriminator architecture has fewer parameters
    than a full-image discriminator and can work on arbitrarily-sized images
    in a fully convolutional fashion.</p>

<pre><code>[n_layers]: With this mode, you can specify the number of conv layers in the discriminator
with the parameter &lt;n_layers_D&gt; (default=3 as used in [basic] (PatchGAN).)

[pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.
It encourages greater color diversity but has no effect on spatial statistics.

</code></pre>
<p>The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="GANLoss" class="doc_header"><code>class</code> <code>GANLoss</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>GANLoss</code>(<strong><code>gan_mode</code></strong>, <strong><code>target_real_label</code></strong>=<em><code>1.0</code></em>, <strong><code>target_fake_label</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Define different GAN objectives.</p>
<p>The GANLoss class abstracts away the need to create the target label tensor
that has the same size as the input.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="cal_gradient_penalty" class="doc_header"><code>cal_gradient_penalty</code><a href="__main__.py#L279" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>cal_gradient_penalty</code>(<strong><code>netD</code></strong>, <strong><code>real_data</code></strong>, <strong><code>fake_data</code></strong>, <strong><code>device</code></strong>, <strong><code>type</code></strong>=<em><code>'mixed'</code></em>, <strong><code>constant</code></strong>=<em><code>1.0</code></em>, <strong><code>lambda_gp</code></strong>=<em><code>10.0</code></em>)</p>
</blockquote>
<p>Calculate the gradient penalty loss, used in WGAN-GP paper <a href="https://arxiv.org/abs/1704.00028">https://arxiv.org/abs/1704.00028</a></p>
<p>Arguments:
    netD (network)              -- discriminator network
    real_data (tensor array)    -- real images
    fake_data (tensor array)    -- generated images from the generator
    device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')
    type (str)                  -- if we mix real and fake data or not [real | fake | mixed].
    constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2
    lambda_gp (float)           -- weight for this loss</p>
<p>Returns the gradient penalty loss</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ResnetGenerator" class="doc_header"><code>class</code> <code>ResnetGenerator</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ResnetGenerator</code>(<strong><code>input_nc</code></strong>, <strong><code>output_nc</code></strong>, <strong><code>ngf</code></strong>=<em><code>64</code></em>, <strong><code>norm_layer</code></strong>=<em><code>'BatchNorm2d'</code></em>, <strong><code>use_dropout</code></strong>=<em><code>False</code></em>, <strong><code>n_blocks</code></strong>=<em><code>6</code></em>, <strong><code>padding_type</code></strong>=<em><code>'reflect'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.</p>
<p>We adapt Torch code and idea from Justin Johnson's neural style transfer project(<a href="https://github.com/jcjohnson/fast-neural-style">https://github.com/jcjohnson/fast-neural-style</a>)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ResnetBlock" class="doc_header"><code>class</code> <code>ResnetBlock</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ResnetBlock</code>(<strong><code>dim</code></strong>, <strong><code>padding_type</code></strong>, <strong><code>norm_layer</code></strong>, <strong><code>use_dropout</code></strong>, <strong><code>use_bias</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Define a Resnet block</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="UnetGenerator" class="doc_header"><code>class</code> <code>UnetGenerator</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>UnetGenerator</code>(<strong><code>input_nc</code></strong>, <strong><code>output_nc</code></strong>, <strong><code>num_downs</code></strong>, <strong><code>ngf</code></strong>=<em><code>64</code></em>, <strong><code>norm_layer</code></strong>=<em><code>'BatchNorm2d'</code></em>, <strong><code>use_dropout</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Create a Unet-based generator</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="UnetSkipConnectionBlock" class="doc_header"><code>class</code> <code>UnetSkipConnectionBlock</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>UnetSkipConnectionBlock</code>(<strong><code>outer_nc</code></strong>, <strong><code>inner_nc</code></strong>, <strong><code>input_nc</code></strong>=<em><code>None</code></em>, <strong><code>submodule</code></strong>=<em><code>None</code></em>, <strong><code>outermost</code></strong>=<em><code>False</code></em>, <strong><code>innermost</code></strong>=<em><code>False</code></em>, <strong><code>norm_layer</code></strong>=<em><code>'BatchNorm2d'</code></em>, <strong><code>use_dropout</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Defines the Unet submodule with skip connection.
X -------------------identity----------------------
|-- downsampling -- |submodule| -- upsampling --|</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="NLayerDiscriminator" class="doc_header"><code>class</code> <code>NLayerDiscriminator</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>NLayerDiscriminator</code>(<strong><code>input_nc</code></strong>, <strong><code>ndf</code></strong>=<em><code>64</code></em>, <strong><code>n_layers</code></strong>=<em><code>3</code></em>, <strong><code>norm_layer</code></strong>=<em><code>'BatchNorm2d'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Defines a PatchGAN discriminator</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PixelDiscriminator" class="doc_header"><code>class</code> <code>PixelDiscriminator</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PixelDiscriminator</code>(<strong><code>input_nc</code></strong>, <strong><code>ndf</code></strong>=<em><code>64</code></em>, <strong><code>norm_layer</code></strong>=<em><code>'BatchNorm2d'</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Defines a 1x1 PatchGAN discriminator (pixelGAN)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

