{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases Callback\n",
    "> Defines a fastai Callback for specifically tracking image-to-image translation experiments in Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tracking.wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import wandb\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.callback.wandb import _format_metadata, _format_config\n",
    "from fastai.basics import *\n",
    "from fastai.vision.gan import *\n",
    "from upit.models.cyclegan import *\n",
    "from upit.data.unpaired import *\n",
    "from upit.train.cyclegan import *\n",
    "from upit.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SaveModelAtEndCallback(Callback):\n",
    "    def __init__(self, fname='model', with_opt=False): store_attr()\n",
    "    def _save(self, name): self.last_saved_path = self.learn.save(name, with_opt=self.with_opt)\n",
    "    def after_fit(self, **kwargs): self._save(f'{self.fname}')\n",
    "    @property\n",
    "    def name(self): return \"save_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def log_dataset(main_path, folder_names=None, name=None, metadata={}, description='raw dataset'):\n",
    "    \"Log dataset folder\"\n",
    "    # Check if wandb.init has been called in case datasets are logged manually\n",
    "    if wandb.run is None:\n",
    "        raise ValueError('You must call wandb.init() before log_dataset()')\n",
    "    path = Path(main_path)\n",
    "    if not path.is_dir():\n",
    "        raise f'path must be a valid directory: {path}'\n",
    "    name = ifnone(name, path.name)\n",
    "    _format_metadata(metadata)\n",
    "    artifact_dataset = wandb.Artifact(name=name, type='dataset', metadata=metadata, description=description)\n",
    "    # log everything in folder_names\n",
    "    if not folder_names: folder_names = [p.name for p in path.ls() if p.is_dir()]\n",
    "    for p in path.ls():\n",
    "        if p.is_dir():\n",
    "            if p.name in folder_names and p.name != 'models': artifact_dataset.add_dir(str(p.resolve()), name=p.name)\n",
    "        else: artifact_dataset.add_file(str(p.resolve()))\n",
    "    wandb.run.use_artifact(artifact_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UPITWandbCallback(Callback):\n",
    "    \"Saves model topology, losses & metrics\"\n",
    "    remove_on_fetch,order = True,Recorder.order+1\n",
    "    # Record if watch has been called previously (even in another instance)\n",
    "    _wandb_watch_called = False\n",
    "\n",
    "    def __init__(self, log=\"gradients\", log_preds=True, log_model=True, log_dataset=False, folder_names=None, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True):\n",
    "        # Check if wandb.init has been called\n",
    "        if wandb.run is None:\n",
    "            raise ValueError('You must call wandb.init() before WandbCallback()')\n",
    "        # W&B log step\n",
    "        self._wandb_step = wandb.run.step - 1  # -1 except if the run has previously logged data (incremented at each batch)\n",
    "        self._wandb_epoch = 0 if not(wandb.run.step) else math.ceil(wandb.run.summary['epoch']) # continue to next epoch\n",
    "        store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Call watch method to log model topology, gradients & weights\"\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n",
    "        if not self.run: return\n",
    "\n",
    "        # Log config parameters\n",
    "        log_config = self.learn.gather_args()\n",
    "        _format_config(log_config)\n",
    "        try:\n",
    "            wandb.config.update(log_config, allow_val_change=True)\n",
    "        except Exception as e:\n",
    "            print(f'WandbCallback could not log config parameters -> {e}')\n",
    "\n",
    "        if not WandbCallback._wandb_watch_called:\n",
    "            WandbCallback._wandb_watch_called = True\n",
    "            # Logs model topology and optionally gradients and weights\n",
    "            wandb.watch(self.learn.model, log=self.log)\n",
    "\n",
    "       \n",
    "        # log dataset\n",
    "        assert isinstance(self.log_dataset, (str, Path, bool)), 'log_dataset must be a path or a boolean'\n",
    "        if self.log_dataset is True:\n",
    "            if Path(self.dls.path) == Path('.'):\n",
    "                print('WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\"')\n",
    "                self.log_dataset = False\n",
    "            else:\n",
    "                self.log_dataset = self.dls.path\n",
    "        \n",
    "        if self.log_dataset:\n",
    "            self.log_dataset = Path(self.log_dataset)\n",
    "            assert self.log_dataset.is_dir(), f'log_dataset must be a valid directory: {self.log_dataset}'\n",
    "            metadata = {'path relative to learner': os.path.relpath(self.log_dataset, self.learn.path)}\n",
    "            if self.folder_names:\n",
    "                assert isinstance(self.folder_names, list), 'folder_names must be a list of folder names as strings'\n",
    "                for name in self.folder_names: assert isinstance(name, str), 'the elements of folder_names must be strings'\n",
    "            log_dataset(main_path=self.log_dataset, folder_names=self.folder_names, name=self.dataset_name, metadata=metadata)\n",
    "\n",
    "\n",
    "        # log model\n",
    "        if self.log_model and not hasattr(self, 'save_model'):\n",
    "            print('Adding SaveModelAtEndCallback()')\n",
    "            self.learn.add_cb(SaveModelAtEndCallback())\n",
    "            self.add_save_model = True\n",
    "        else: self.add_save_model = False\n",
    "\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                if not self.valid_dl:\n",
    "                    if not len(self.dls.valid_ds):\n",
    "                        print('Saving training set predictions')\n",
    "                        #Initializes the batch watched\n",
    "                        wandbRandom = random.Random(self.seed)  # For repeatability\n",
    "                        self.n_preds = min(self.n_preds, len(self.dls.train_ds))\n",
    "                        idxs = wandbRandom.sample(range(len(self.dls.train_ds)), self.n_preds)\n",
    "                        test_items = [getattr(self.dls.train_ds.items, 'iloc', self.dls.train_ds.items)[i] for i in idxs]\n",
    "                        self.preds_dl = self.dls.test_dl(test_items, with_labels=True)\n",
    "                        \n",
    "                else: self.preds_dl = self.valid_dl\n",
    "                self.learn.add_cb(FetchPredsCallback(dl=self.preds_dl, with_input=True, with_decoded=True, reorder=self.reorder))\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to prepare a DataLoader for logging prediction samples -> {e}')\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Log hyper-parameters and training loss\"\n",
    "        if self.training:\n",
    "            self._wandb_step += 1\n",
    "            self._wandb_epoch += 1/self.n_iter\n",
    "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
    "\n",
    "            wandb.log({'epoch': self._wandb_epoch, 'train_loss': float(to_detach(self.smooth_loss.clone())), \n",
    "                       'raw_loss': float(to_detach(self.loss.clone())), **hypers}, step=self._wandb_step)\n",
    "\n",
    "    def log_predictions(self, preds):\n",
    "        raise NotImplementedError(\"To be implemented\")\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Log validation loss and custom metrics & log prediction samples\"\n",
    "        # Correct any epoch rounding error and overwrite value\n",
    "        self._wandb_epoch = round(self._wandb_epoch)\n",
    "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
    "        # Log sample predictions\n",
    "        if self.log_preds:\n",
    "            try:\n",
    "                self.log_predictions(self.learn.fetch_preds.preds)\n",
    "            except Exception as e:\n",
    "                self.log_preds = False\n",
    "                print(f'WandbCallback was not able to get prediction samples -> {e}')\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.log_model:\n",
    "            if self.save_model.last_saved_path is None:\n",
    "                print('WandbCallback could not retrieve a model to upload')\n",
    "            else:\n",
    "                metadata = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n",
    "                log_model(self.save_model.last_saved_path, metadata=metadata)\n",
    "        self.run = True\n",
    "        self.learn.remove_cb(FetchPredsCallback)\n",
    "        self.learn.remove_cb(SaveModelAtEndCallback)\n",
    "        wandb.log({})  # ensure sync of last step\n",
    "        self._wandb_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtmabraham\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tmabraham/UPIT/nbs/wandb/run-20220605_010714-yidpx9j2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tmabraham/UPIT-nbs/runs/yidpx9j2\" target=\"_blank\">colorful-dragon-27</a></strong> to <a href=\"https://wandb.ai/tmabraham/UPIT-nbs\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/home/tmabraham/.fastai/data/horse2zebra/trainA)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not gather input dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done. 0.3s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/home/tmabraham/.fastai/data/horse2zebra/trainB)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding SaveModelAtEndCallback()\n",
      "Saving training set predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>id_loss_A</th>\n",
       "      <th>id_loss_B</th>\n",
       "      <th>gen_loss_A</th>\n",
       "      <th>gen_loss_B</th>\n",
       "      <th>cyc_loss_A</th>\n",
       "      <th>cyc_loss_B</th>\n",
       "      <th>D_A_loss</th>\n",
       "      <th>D_B_loss</th>\n",
       "      <th>frechet_inception_distance</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.867505</td>\n",
       "      <td>1.454771</td>\n",
       "      <td>1.509735</td>\n",
       "      <td>0.430915</td>\n",
       "      <td>0.450728</td>\n",
       "      <td>3.095023</td>\n",
       "      <td>3.188569</td>\n",
       "      <td>0.400037</td>\n",
       "      <td>0.400273</td>\n",
       "      <td>91.447801</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.506508</td>\n",
       "      <td>1.120648</td>\n",
       "      <td>1.158904</td>\n",
       "      <td>0.289497</td>\n",
       "      <td>0.295813</td>\n",
       "      <td>2.388297</td>\n",
       "      <td>2.494975</td>\n",
       "      <td>0.258037</td>\n",
       "      <td>0.257544</td>\n",
       "      <td>94.591660</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmabraham/anaconda3/envs/UPIT/lib/python3.9/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandbCallback was not able to get prediction samples -> To be implemented\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='107.959 MB of 107.959 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>D_A_loss</td><td>█▁</td></tr><tr><td>D_B_loss</td><td>█▁</td></tr><tr><td>cyc_loss_A</td><td>█▁</td></tr><tr><td>cyc_loss_B</td><td>█▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>frechet_inception_distance</td><td>▁█</td></tr><tr><td>gen_loss_A</td><td>█▁</td></tr><tr><td>gen_loss_B</td><td>█▁</td></tr><tr><td>id_loss_A</td><td>█▁</td></tr><tr><td>id_loss_B</td><td>█▁</td></tr><tr><td>lr_0</td><td>██████████████████████▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>raw_loss</td><td>█▆▅▄▅▃▃▃▂▄▂▃▃▃▂▃▂▂▂▂▂▃▂▃▂▂▂▁▁▂▂▂▂▂▁▂▂▂▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>D_A_loss</td><td>0.25804</td></tr><tr><td>D_B_loss</td><td>0.25754</td></tr><tr><td>cyc_loss_A</td><td>2.3883</td></tr><tr><td>cyc_loss_B</td><td>2.49498</td></tr><tr><td>epoch</td><td>2</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>frechet_inception_distance</td><td>94.59166</td></tr><tr><td>gen_loss_A</td><td>0.2895</td></tr><tr><td>gen_loss_B</td><td>0.29581</td></tr><tr><td>id_loss_A</td><td>1.12065</td></tr><tr><td>id_loss_B</td><td>1.1589</td></tr><tr><td>lr_0</td><td>1e-05</td></tr><tr><td>mom_0</td><td>0.5</td></tr><tr><td>raw_loss</td><td>6.90033</td></tr><tr><td>sqr_mom_0</td><td>0.999</td></tr><tr><td>train_loss</td><td>8.50651</td></tr><tr><td>wd_0</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">colorful-dragon-27</strong>: <a href=\"https://wandb.ai/tmabraham/UPIT-nbs/runs/yidpx9j2\" target=\"_blank\">https://wandb.ai/tmabraham/UPIT-nbs/runs/yidpx9j2</a><br/>Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220605_010714-yidpx9j2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cuda\n",
    "import tempfile\n",
    "\n",
    "horse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')\n",
    "folders = horse2zebra.ls().sorted()\n",
    "trainA_path = folders[2]\n",
    "trainB_path = folders[3]\n",
    "testA_path = folders[0]\n",
    "testB_path = folders[1]\n",
    "dls = get_dls(trainA_path, trainB_path, num_A=100, num_B=100, load_size=286)\n",
    "\n",
    "#os.environ['WANDB_MODE'] = 'dryrun' # run offline\n",
    "wandb.init()\n",
    "cycle_gan = CycleGAN(3,3,64)\n",
    "learn = cycle_learner(dls, cycle_gan,opt_func=partial(Adam,mom=0.5,sqr_mom=0.999),\n",
    "                    metrics=[FrechetInceptionDistance()])\n",
    "\n",
    "learn.fit_flat_lin(1,1,2e-4,cbs=[UPITWandbCallback(log_preds=True, log_model=True, log_dataset=horse2zebra, folder_names=[trainA_path.name,trainB_path.name])])\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "_,_,preds = learn.get_preds(dl=[b], with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_models.cyclegan.ipynb.\n",
      "Converted 01b_models.junyanz.ipynb.\n",
      "Converted 02_data.unpaired.ipynb.\n",
      "Converted 03_train.cyclegan.ipynb.\n",
      "Converted 04_inference.cyclegan.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_tracking.wandb.ipynb.\n",
      "Converted 07_models.dualgan.ipynb.\n",
      "Converted 08_train.dualgan.ipynb.\n",
      "Converted 09_models.ganilla.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
