{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANILLA model\n",
    "\n",
    "> Defines the GANILLA model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.ganilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.basics import *\n",
    "from typing import List\n",
    "from fastai.vision.gan import *\n",
    "from upit.models.cyclegan import *\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the generator that was introduced in the [GANILLA paper](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BasicBlock_Ganilla(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, use_dropout, stride=1):\n",
    "        super(BasicBlock_Ganilla, self).__init__()\n",
    "        self.rp1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=0, bias=False)\n",
    "        self.bn1 = nn.InstanceNorm2d(planes)\n",
    "        self.use_dropout = use_dropout\n",
    "        if use_dropout:\n",
    "            self.dropout = nn.Dropout(use_dropout)\n",
    "        self.rp2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.InstanceNorm2d(planes)\n",
    "        self.out_planes = planes\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.InstanceNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "            self.final_conv = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(self.expansion * planes * 2, self.expansion * planes, kernel_size=3, stride=1,\n",
    "                                        padding=0, bias=False),\n",
    "                nn.InstanceNorm2d(self.expansion * planes)\n",
    "            )\n",
    "        else:\n",
    "            self.final_conv = nn.Sequential(\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.Conv2d(planes*2, planes, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "                nn.InstanceNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(self.rp1(x))))\n",
    "        if self.use_dropout:\n",
    "            out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(self.rp2(out)))\n",
    "        inputt = self.shortcut(x)\n",
    "        catted = torch.cat((out, inputt), 1)\n",
    "        out = self.final_conv(catted)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PyramidFeatures(nn.Module):\n",
    "    def __init__(self, C2_size, C3_size, C4_size, C5_size, fpn_weights, feature_size=128):\n",
    "        super(PyramidFeatures, self).__init__()\n",
    "\n",
    "        self.sum_weights = fpn_weights #[1.0, 0.5, 0.5, 0.5]\n",
    "\n",
    "        # upsample C5 to get P5 from the FPN paper\n",
    "        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp1 = nn.ReflectionPad2d(1)\n",
    "        #self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        # add P5 elementwise to C4\n",
    "        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp2 = nn.ReflectionPad2d(1)\n",
    "        #self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        # add P4 elementwise to C3\n",
    "        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp3 = nn.ReflectionPad2d(1)\n",
    "        #self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        self.P2_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.rp4 = nn.ReflectionPad2d(1)\n",
    "        self.P2_2 = nn.Conv2d(int(feature_size), int(feature_size/2), kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        #self.P1_1 = nn.Conv2d(feature_size, feature_size, kernel_size=1, stride=1, padding=0)\n",
    "        #self.P1_upsampled = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        #self.rp5 = nn.ReflectionPad2d(1)\n",
    "        #self.P1_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        C2, C3, C4, C5 = inputs\n",
    "\n",
    "        i = 0\n",
    "        P5_x = self.P5_1(C5) * self.sum_weights[i]\n",
    "        P5_upsampled_x = self.P5_upsampled(P5_x)\n",
    "        #P5_x = self.rp1(P5_x)\n",
    "        # #P5_x = self.P5_2(P5_x)\n",
    "        i += 1\n",
    "        P4_x = self.P4_1(C4) * self.sum_weights[i]\n",
    "        P4_x = P5_upsampled_x + P4_x\n",
    "        P4_upsampled_x = self.P4_upsampled(P4_x)\n",
    "        #P4_x = self.rp2(P4_x)\n",
    "        # #P4_x = self.P4_2(P4_x)\n",
    "        i += 1\n",
    "        P3_x = self.P3_1(C3) * self.sum_weights[i]\n",
    "        P3_x = P3_x + P4_upsampled_x\n",
    "        P3_upsampled_x = self.P3_upsampled(P3_x)\n",
    "        #P3_x = self.rp3(P3_x)\n",
    "        #P3_x = self.P3_2(P3_x)\n",
    "        i += 1\n",
    "        P2_x = self.P2_1(C2) * self.sum_weights[i]\n",
    "        P2_x = P2_x * self.sum_weights[2] + P3_upsampled_x\n",
    "        P2_upsampled_x = self.P2_upsampled(P2_x)\n",
    "        P2_x = self.rp4(P2_upsampled_x)\n",
    "        P2_x = self.P2_2(P2_x)\n",
    "\n",
    "        return P2_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf, use_dropout, fpn_weights, block, layers):\n",
    "        self.inplanes = ngf\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # first conv\n",
    "        self.pad1 = nn.ReflectionPad2d(input_nc)\n",
    "        self.conv1 = nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=True)\n",
    "        self.in1 = nn.InstanceNorm2d(ngf)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pad2 = nn.ReflectionPad2d(1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "        # Output layer\n",
    "        self.pad3 = nn.ReflectionPad2d(output_nc)\n",
    "        self.conv2 = nn.Conv2d(64, output_nc, 7)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "        if block == BasicBlock_Ganilla:\n",
    "            # residuals\n",
    "            self.layer1 = self._make_layer_ganilla(block, 64, layers[0], use_dropout, stride=1)\n",
    "            self.layer2 = self._make_layer_ganilla(block, 128, layers[1], use_dropout, stride=2)\n",
    "            self.layer3 = self._make_layer_ganilla(block, 128, layers[2], use_dropout, stride=2)\n",
    "            self.layer4 = self._make_layer_ganilla(block, 256, layers[3], use_dropout, stride=2)\n",
    "\n",
    "            fpn_sizes = [self.layer1[layers[0] - 1].conv2.out_channels,\n",
    "                         self.layer2[layers[1] - 1].conv2.out_channels,\n",
    "                         self.layer3[layers[2] - 1].conv2.out_channels,\n",
    "                         self.layer4[layers[3] - 1].conv2.out_channels]\n",
    "\n",
    "        else:\n",
    "            print(\"This block type is not supported\")\n",
    "            sys.exit()\n",
    "\n",
    "        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2], fpn_sizes[3], fpn_weights)\n",
    "\n",
    "    \n",
    "    def _make_layer_ganilla(self, block, planes, blocks, use_dropout, stride=1):\n",
    "        strides = [stride] + [1] * (blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inplanes, planes, use_dropout, stride))\n",
    "            self.inplanes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        img_batch = inputs\n",
    "\n",
    "        x = self.pad1(img_batch)\n",
    "        x = self.conv1(x)\n",
    "        x = self.in1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pad2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        out = self.fpn([x1, x2, x3, x4]) # use all resnet layers\n",
    "\n",
    "        out = self.pad3(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                torch.nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    net.apply(init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ganilla_generator(input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0, 1.0, 1.0], init_type='normal', gain=0.02, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 GANILLA generator.\"\"\"\n",
    "    model = ResNet(input_nc, output_nc, ngf, drop, fpn_weights, BasicBlock_Ganilla, [2, 2, 2, 2],  **kwargs)\n",
    "    init_weights(model,init_type='normal', gain=gain)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for a few things:\n",
    "1. The generator can indeed be initialized correctly\n",
    "2. A random image can be passed into the model successfully with the correct size output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torch.randn(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ganilla_generator(3,3,64,0.5)\n",
    "with torch.no_grad():\n",
    "    out1 = m(img1)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group two discriminators and two generators in a single model, then a `Callback` (defined in `02_cyclegan_training.ipynb`) will take care of training them properly. The discriminator and training loop is the same as CycleGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANILLA(nn.Module, PyTorchModelHubMixin):\n",
    "    \"\"\"\n",
    "    GANILLA model. \\n\n",
    "    When called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). \n",
    "    Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\n",
    "\n",
    "    Attributes: \\n\n",
    "    `G_A` (`nn.Module`): takes real input B and generates fake input A \\n\n",
    "    `G_B` (`nn.Module`): takes real input A and generates fake input B \\n\n",
    "    `D_A` (`nn.Module`): trained to make the difference between real input A and fake input A \\n\n",
    "    `D_B` (`nn.Module`): trained to make the difference between real input B and fake input B \\n\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in:int=3, ch_out:int=3, n_features:int=64, disc_layers:int=3, lsgan:bool=True, \n",
    "                 drop:float=0., norm_layer:nn.Module=None, fpn_weights:list=[1.0, 1.0, 1.0, 1.0], init_type:str='normal', gain:float=0.02,**kwargs):\n",
    "        \"\"\"\n",
    "        Constructor for GANILLA model.\n",
    "        \n",
    "        Arguments: \\n\n",
    "        `ch_in` (`int`): Number of input channels (default=3) \\n\n",
    "        `ch_out` (`int`): Number of output channels (default=3) \\n\n",
    "        `n_features` (`int`): Number of input features (default=64) \\n\n",
    "        `disc_layers` (`int`): Number of discriminator layers (default=3) \\n\n",
    "        `lsgan` (`bool`): LSGAN training objective (output unnormalized float) or not? (default=True) \\n\n",
    "        `drop` (`float`): Level of dropout (default=0) \\n\n",
    "        `norm_layer` (`nn.Module`): Type of normalization layer to use in the discriminator (default=None)\n",
    "        `fpn_weights` (`list`): Weights for feature pyramid network (default=[1.0, 1.0, 1.0, 1.0]) \\n\n",
    "        `init_type` (`str`): Type of initialization (default='normal') \\n\n",
    "        `gain` (`float`): Gain for initialization (default=0.02)\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        #G_A: takes real input B and generates fake input A\n",
    "        #G_B: takes real input A and generates fake input B\n",
    "        #D_A: trained to make the difference between real input A and fake input A\n",
    "        #D_B: trained to make the difference between real input B and fake input B\n",
    "        self.D_A = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.D_B = discriminator(ch_in, n_features, disc_layers, norm_layer, sigmoid=not lsgan)\n",
    "        self.G_A = ganilla_generator(ch_in, ch_out, n_features, drop, fpn_weights, init_type, gain, **kwargs)\n",
    "        self.G_B = ganilla_generator(ch_in, ch_out, n_features, drop, fpn_weights, init_type, gain, **kwargs)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B.\"\"\"\n",
    "        real_A, real_B = input\n",
    "        fake_A, fake_B = self.G_A(real_B), self.G_B(real_A)\n",
    "        idt_A, idt_B = self.G_A(real_A), self.G_B(real_B) #Needed for the identity loss during training.\n",
    "        return [fake_A, fake_B, idt_A, idt_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"GANILLA\" class=\"doc_header\"><code>class</code> <code>GANILLA</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>GANILLA</code>(**`ch_in`**:`int`=*`3`*, **`ch_out`**:`int`=*`3`*, **`n_features`**:`int`=*`64`*, **`disc_layers`**:`int`=*`3`*, **`lsgan`**:`bool`=*`True`*, **`drop`**:`float`=*`0.0`*, **`norm_layer`**:`Module`=*`None`*, **`fpn_weights`**:`list`=*`[1.0, 1.0, 1.0, 1.0]`*, **`init_type`**:`str`=*`'normal'`*, **`gain`**:`float`=*`0.02`*, **\\*\\*`kwargs`**) :: `Module`\n",
       "\n",
       "GANILLA model. \n",
       "\n",
       "When called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). \n",
       "Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\n",
       "\n",
       "Attributes: \n",
       "\n",
       "`G_A` (`nn.Module`): takes real input B and generates fake input A \n",
       "\n",
       "`G_B` (`nn.Module`): takes real input A and generates fake input B \n",
       "\n",
       "`D_A` (`nn.Module`): trained to make the difference between real input A and fake input A \n",
       "\n",
       "`D_B` (`nn.Module`): trained to make the difference between real input B and fake input B "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GANILLA,title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GANILLA.__init__\" class=\"doc_header\"><code>GANILLA.__init__</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GANILLA.__init__</code>(**`ch_in`**:`int`=*`3`*, **`ch_out`**:`int`=*`3`*, **`n_features`**:`int`=*`64`*, **`disc_layers`**:`int`=*`3`*, **`lsgan`**:`bool`=*`True`*, **`drop`**:`float`=*`0.0`*, **`norm_layer`**:`Module`=*`None`*, **`fpn_weights`**:`list`=*`[1.0, 1.0, 1.0, 1.0]`*, **`init_type`**:`str`=*`'normal'`*, **`gain`**:`float`=*`0.02`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Constructor for GANILLA model.\n",
       "\n",
       "Arguments: \n",
       "\n",
       "`ch_in` (`int`): Number of input channels (default=3) \n",
       "\n",
       "`ch_out` (`int`): Number of output channels (default=3) \n",
       "\n",
       "`n_features` (`int`): Number of input features (default=64) \n",
       "\n",
       "`disc_layers` (`int`): Number of discriminator layers (default=3) \n",
       "\n",
       "`lsgan` (`bool`): LSGAN training objective (output unnormalized float) or not? (default=True) \n",
       "\n",
       "`drop` (`float`): Level of dropout (default=0) \n",
       "\n",
       "`norm_layer` (`nn.Module`): Type of normalization layer to use in the discriminator (default=None)\n",
       "`fpn_weights` (`list`): Weights for feature pyramid network (default=[1.0, 1.0, 1.0, 1.0]) \n",
       "\n",
       "`init_type` (`str`): Type of initialization (default='normal') \n",
       "\n",
       "`gain` (`float`): Gain for initialization (default=0.02)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GANILLA.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GANILLA.forward\" class=\"doc_header\"><code>GANILLA.forward</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GANILLA.forward</code>(**`input`**)\n",
       "\n",
       "Forward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(GANILLA.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick model tests\n",
    "\n",
    "Again, let's check that the model can be called sucsessfully and outputs the correct shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ganilla_model = GANILLA()\n",
    "img1 = torch.randn(4,3,256,256)\n",
    "img2 = torch.randn(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 8.36 s, total: 50.5 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad(): ganilla_output = ganilla_model((img1,img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(ganilla_output),4)\n",
    "for output_batch in ganilla_output:\n",
    "    test_eq(output_batch.shape,img1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/tmabraham/upit-ganilla-test into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950fb70f83c46b5ab964ed4ae0689f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 3.34k/76.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/tmabraham/upit-ganilla-test\n",
      "   264c8d0..38cafb3  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/tmabraham/upit-ganilla-test/commit/38cafb3d4cca069313b8ed03d88ecc88db28f3d5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganilla_model.push_to_hub('upit-ganilla-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a3496dba784fb99935e53c5e98c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/80.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GANILLA(\n",
       "  (D_A): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (D_B): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (G_A): ResNet(\n",
       "    (pad1): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (in1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (pad2): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pad3): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv2): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (tanh): Tanh()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): PyramidFeatures(\n",
       "      (P5_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P4_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P3_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P3_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P2_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P2_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (rp4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (P2_2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (G_B): ResNet(\n",
       "    (pad1): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (in1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (pad2): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pad3): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv2): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (tanh): Tanh()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): PyramidFeatures(\n",
       "      (P5_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P4_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P3_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P3_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P2_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P2_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (rp4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (P2_2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_output\n",
    "ganilla_model.from_pretrained('tmabraham/upit-ganilla-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_models.cyclegan.ipynb.\n",
      "Converted 01b_models.junyanz.ipynb.\n",
      "Converted 02_data.unpaired.ipynb.\n",
      "Converted 03_train.cyclegan.ipynb.\n",
      "Converted 04_inference.cyclegan.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_tracking.wandb.ipynb.\n",
      "Converted 07_models.dualgan.ipynb.\n",
      "Converted 08_train.dualgan.ipynb.\n",
      "Converted 09_models.ganilla.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
