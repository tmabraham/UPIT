{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN batch inference\n",
    "> Provides batch inference functionality for the CycleGAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp inference.cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from upit.models.cyclegan import *\n",
    "from upit.train.cyclegan import *\n",
    "from upit.data.unpaired import *\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import glob\n",
    "import tqdm\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are given a test set as a folder, we can use the `get_preds_cyclegan` function defined below to perform batch inference on the images in the folder and save the predictions.\n",
    "\n",
    "I found it easier to write my own inference functionality for the custom CycleGAN model than fastai's built-in functionality. \n",
    "\n",
    "I define a PyTorch Dataset that can be used for inference just by passing in the folder with the image files for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class that can be created from a folder `path` of images, for the sole purpose of inference. Optional `transforms`\n",
    "    can be provided.\n",
    "    \n",
    "    Attributes: \\n\n",
    "    `self.files`: A list of the filenames in the folder. \\n\n",
    "    `self.totensor`: `torchvision.transforms.ToTensor` transform. \\n\n",
    "    `self.transform`: The transforms passed in as `transforms` to the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, path,transforms=None):\n",
    "        \"\"\"Constructor for this PyTorch Dataset, need to pass the `path`\"\"\"\n",
    "        self.files = glob.glob(path+'/*')\n",
    "        self.totensor = torchvision.transforms.ToTensor()\n",
    "        if transforms:\n",
    "            self.transform = torchvision.transforms.Compose(transforms)\n",
    "        else:\n",
    "            self.transform = lambda x: x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PIL.Image.open(self.files[idx % len(self.files)])\n",
    "        image = self.totensor(image)\n",
    "        image = self.transform(image)\n",
    "        return self.files[idx], image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"FolderDataset\" class=\"doc_header\"><code>class</code> <code>FolderDataset</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>FolderDataset</code>(**`path`**, **`transforms`**=*`None`*) :: `Dataset`\n",
       "\n",
       "A PyTorch Dataset class that can be created from a folder `path` of images, for the sole purpose of inference. Optional `transforms`\n",
       "can be provided.\n",
       "\n",
       "Attributes: \n",
       "\n",
       "`self.files`: A list of the filenames in the folder. \n",
       "\n",
       "`self.totensor`: `torchvision.transforms.ToTensor` transform. \n",
       "\n",
       "`self.transform`: The transforms passed in as `transforms` to the constructor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FolderDataset,title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function for making the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_dataset(test_path,bs=4,num_workers=4):\n",
    "    \"A helper function for getting a DataLoader for images in the folder `test_path`, with batch size `bs`, and number of workers `num_workers`\"\n",
    "    dataset = FolderDataset(\n",
    "            path=test_path,\n",
    "            transforms=[torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "        ) \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=bs,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"load_dataset\" class=\"doc_header\"><code>load_dataset</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>load_dataset</code>(**`test_path`**, **`bs`**=*`4`*, **`num_workers`**=*`4`*)\n",
       "\n",
       "A helper function for getting a DataLoader for images in the folder `test_path`, with batch size `bs`, and number of workers `num_workers`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(load_dataset,title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_preds_cyclegan(learn,test_path,pred_path,bs=4,num_workers=4,suffix='tif'):\n",
    "    \"\"\"\n",
    "    A prediction function that takes the Learner object `learn` with the trained model, the `test_path` folder with the images to perform \n",
    "    batch inference on, and the output folder `pred_path` where the predictions will be saved, with a batch size `bs`, `num_workers`, \n",
    "    and suffix of the prediction images `suffix` (default='png'). \n",
    "    \"\"\"\n",
    "    \n",
    "    assert os.path.exists(test_path)\n",
    "    \n",
    "    if not os.path.exists(pred_path):\n",
    "        os.mkdir(pred_path)\n",
    "    \n",
    "    test_dl = load_dataset(test_path,bs,num_workers)\n",
    "    model = learn.model.G_B.cuda()\n",
    "    for i, xb in tqdm.tqdm(enumerate(test_dl),total=len(test_dl)):\n",
    "        fn, im = xb\n",
    "        preds = (model(im.cuda())/2 + 0.5)\n",
    "        for i in range(len(fn)):\n",
    "            new_fn = os.path.join(pred_path,'.'.join([os.path.basename(fn[i]).split('.')[0]+'_fakeB',suffix]))                  \n",
    "            torchvision.utils.save_image(preds[i],new_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"get_preds_cyclegan\" class=\"doc_header\"><code>get_preds_cyclegan</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>get_preds_cyclegan</code>(**`learn`**, **`test_path`**, **`pred_path`**, **`bs`**=*`4`*, **`num_workers`**=*`4`*, **`suffix`**=*`'tif'`*)\n",
       "\n",
       "A prediction function that takes the Learner object `learn` with the trained model, the `test_path` folder with the images to perform \n",
       "batch inference on, and the output folder `pred_path` where the predictions will be saved, with a batch size `bs`, `num_workers`, \n",
       "and suffix of the prediction images `suffix` (default='png'). "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_preds_cyclegan,title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse2zebra = untar_data('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = horse2zebra.ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainA_path = folders[2]\n",
    "trainB_path = folders[3]\n",
    "testA_path = folders[0]\n",
    "testB_path = folders[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda\n",
    "dls = get_dls(trainA_path, trainB_path,load_size=286)\n",
    "cycle_gan = CycleGAN(3,3,64)\n",
    "learn = cycle_learner(dls, cycle_gan)\n",
    "#learn.load('h2z-200epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 16.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "preds_path = './h2z-preds'\n",
    "get_preds_cyclegan(learn,str(testA_path),preds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_models.cyclegan.ipynb.\n",
      "Converted 01b_models.junyanz.ipynb.\n",
      "Converted 02_data.unpaired.ipynb.\n",
      "Converted 03_train.cyclegan.ipynb.\n",
      "Converted 04_inference.cyclegan.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
