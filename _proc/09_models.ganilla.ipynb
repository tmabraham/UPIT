{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Defines the GANILLA model architecture.\n",
    "output-file: models.ganilla.html\n",
    "title: GANILLA model\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the generator that was introduced in the [GANILLA paper](https://arxiv.org/abs/1703.10593)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L15){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasicBlock_Ganilla\n",
       "\n",
       ">      BasicBlock_Ganilla (in_planes, planes, use_dropout, stride=1)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L15){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BasicBlock_Ganilla\n",
       "\n",
       ">      BasicBlock_Ganilla (in_planes, planes, use_dropout, stride=1)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(BasicBlock_Ganilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PyramidFeatures\n",
       "\n",
       ">      PyramidFeatures (C2_size, C3_size, C4_size, C5_size, fpn_weights,\n",
       ">                       feature_size=128)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L63){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PyramidFeatures\n",
       "\n",
       ">      PyramidFeatures (C2_size, C3_size, C4_size, C5_size, fpn_weights,\n",
       ">                       feature_size=128)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(PyramidFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ResNet\n",
       "\n",
       ">      ResNet (input_nc, output_nc, ngf, use_dropout, fpn_weights, block,\n",
       ">              layers)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ResNet\n",
       "\n",
       ">      ResNet (input_nc, output_nc, ngf, use_dropout, fpn_weights, block,\n",
       ">              layers)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/junyanz.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### init_weights\n",
       "\n",
       ">      init_weights (net, init_type='normal', gain=0.02)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/junyanz.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### init_weights\n",
       "\n",
       ">      init_weights (net, init_type='normal', gain=0.02)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L229){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ganilla_generator\n",
       "\n",
       ">      ganilla_generator (input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0,\n",
       ">                         1.0, 1.0], init_type='normal', gain=0.02, **kwargs)\n",
       "\n",
       "Constructs a ResNet-18 GANILLA generator."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L229){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ganilla_generator\n",
       "\n",
       ">      ganilla_generator (input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0,\n",
       ">                         1.0, 1.0], init_type='normal', gain=0.02, **kwargs)\n",
       "\n",
       "Constructs a ResNet-18 GANILLA generator."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(ganilla_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for a few things:\n",
    "1. The generator can indeed be initialized correctly\n",
    "2. A random image can be passed into the model successfully with the correct size output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "img1 = torch.randn(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ganilla_generator(3,3,64,0.5)\n",
    "with torch.no_grad():\n",
    "    out1 = m(img1)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group two discriminators and two generators in a single model, then a `Callback` (defined in `02_cyclegan_training.ipynb`) will take care of training them properly. The discriminator and training loop is the same as CycleGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L236){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA\n",
       "\n",
       ">      GANILLA (ch_in:int=3, ch_out:int=3, n_features:int=64, disc_layers:int=3,\n",
       ">               lsgan:bool=True, drop:float=0.0,\n",
       ">               norm_layer:torch.nn.modules.module.Module=None,\n",
       ">               fpn_weights:list=[1.0, 1.0, 1.0, 1.0], init_type:str='normal',\n",
       ">               gain:float=0.02, **kwargs)\n",
       "\n",
       "GANILLA model. \n",
       "\n",
       "When called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). \n",
       "Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\n",
       "\n",
       "Attributes: \n",
       "\n",
       "`G_A` (`nn.Module`): takes real input B and generates fake input A \n",
       "\n",
       "`G_B` (`nn.Module`): takes real input A and generates fake input B \n",
       "\n",
       "`D_A` (`nn.Module`): trained to make the difference between real input A and fake input A \n",
       "\n",
       "`D_B` (`nn.Module`): trained to make the difference between real input B and fake input B"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L236){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA\n",
       "\n",
       ">      GANILLA (ch_in:int=3, ch_out:int=3, n_features:int=64, disc_layers:int=3,\n",
       ">               lsgan:bool=True, drop:float=0.0,\n",
       ">               norm_layer:torch.nn.modules.module.Module=None,\n",
       ">               fpn_weights:list=[1.0, 1.0, 1.0, 1.0], init_type:str='normal',\n",
       ">               gain:float=0.02, **kwargs)\n",
       "\n",
       "GANILLA model. \n",
       "\n",
       "When called, takes in input batch of real images from both domains and outputs fake images for the opposite domains (with the generators). \n",
       "Also outputs identity images after passing the images into generators that outputs its domain type (needed for identity loss).\n",
       "\n",
       "Attributes: \n",
       "\n",
       "`G_A` (`nn.Module`): takes real input B and generates fake input A \n",
       "\n",
       "`G_B` (`nn.Module`): takes real input A and generates fake input B \n",
       "\n",
       "`D_A` (`nn.Module`): trained to make the difference between real input A and fake input A \n",
       "\n",
       "`D_B` (`nn.Module`): trained to make the difference between real input B and fake input B"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(GANILLA,title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA.__init__\n",
       "\n",
       ">      GANILLA.__init__ (ch_in:int=3, ch_out:int=3, n_features:int=64,\n",
       ">                        disc_layers:int=3, lsgan:bool=True, drop:float=0.0,\n",
       ">                        norm_layer:torch.nn.modules.module.Module=None,\n",
       ">                        fpn_weights:list=[1.0, 1.0, 1.0, 1.0],\n",
       ">                        init_type:str='normal', gain:float=0.02, **kwargs)\n",
       "\n",
       "Constructor for GANILLA model.\n",
       "\n",
       "Arguments: \n",
       "\n",
       "`ch_in` (`int`): Number of input channels (default=3) \n",
       "\n",
       "`ch_out` (`int`): Number of output channels (default=3) \n",
       "\n",
       "`n_features` (`int`): Number of input features (default=64) \n",
       "\n",
       "`disc_layers` (`int`): Number of discriminator layers (default=3) \n",
       "\n",
       "`lsgan` (`bool`): LSGAN training objective (output unnormalized float) or not? (default=True) \n",
       "\n",
       "`drop` (`float`): Level of dropout (default=0) \n",
       "\n",
       "`norm_layer` (`nn.Module`): Type of normalization layer to use in the discriminator (default=None)\n",
       "`fpn_weights` (`list`): Weights for feature pyramid network (default=[1.0, 1.0, 1.0, 1.0]) \n",
       "\n",
       "`init_type` (`str`): Type of initialization (default='normal') \n",
       "\n",
       "`gain` (`float`): Gain for initialization (default=0.02)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L248){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA.__init__\n",
       "\n",
       ">      GANILLA.__init__ (ch_in:int=3, ch_out:int=3, n_features:int=64,\n",
       ">                        disc_layers:int=3, lsgan:bool=True, drop:float=0.0,\n",
       ">                        norm_layer:torch.nn.modules.module.Module=None,\n",
       ">                        fpn_weights:list=[1.0, 1.0, 1.0, 1.0],\n",
       ">                        init_type:str='normal', gain:float=0.02, **kwargs)\n",
       "\n",
       "Constructor for GANILLA model.\n",
       "\n",
       "Arguments: \n",
       "\n",
       "`ch_in` (`int`): Number of input channels (default=3) \n",
       "\n",
       "`ch_out` (`int`): Number of output channels (default=3) \n",
       "\n",
       "`n_features` (`int`): Number of input features (default=64) \n",
       "\n",
       "`disc_layers` (`int`): Number of discriminator layers (default=3) \n",
       "\n",
       "`lsgan` (`bool`): LSGAN training objective (output unnormalized float) or not? (default=True) \n",
       "\n",
       "`drop` (`float`): Level of dropout (default=0) \n",
       "\n",
       "`norm_layer` (`nn.Module`): Type of normalization layer to use in the discriminator (default=None)\n",
       "`fpn_weights` (`list`): Weights for feature pyramid network (default=[1.0, 1.0, 1.0, 1.0]) \n",
       "\n",
       "`init_type` (`str`): Type of initialization (default='normal') \n",
       "\n",
       "`gain` (`float`): Gain for initialization (default=0.02)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(GANILLA.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L277){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA.forward\n",
       "\n",
       ">      GANILLA.forward (input)\n",
       "\n",
       "Forward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/tmabraham/UPIT/tree/master/blob/master/upit/models/ganilla.py#L277){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### GANILLA.forward\n",
       "\n",
       ">      GANILLA.forward (input)\n",
       "\n",
       "Forward function for CycleGAN model. The input is a tuple of a batch of real images from both domains A and B."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(GANILLA.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick model tests\n",
    "\n",
    "Again, let's check that the model can be called sucsessfully and outputs the correct shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ganilla_model = GANILLA()\n",
    "img1 = torch.randn(4,3,256,256)\n",
    "img2 = torch.randn(4,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 8.36 s, total: 50.5 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): ganilla_output = ganilla_model((img1,img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "test_eq(len(ganilla_output),4)\n",
    "for output_batch in ganilla_output:\n",
    "    test_eq(output_batch.shape,img1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/tmabraham/upit-ganilla-test into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1950fb70f83c46b5ab964ed4ae0689f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 3.34k/76.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/tmabraham/upit-ganilla-test\n",
      "   264c8d0..38cafb3  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/tmabraham/upit-ganilla-test/commit/38cafb3d4cca069313b8ed03d88ecc88db28f3d5'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "ganilla_model.push_to_hub('upit-ganilla-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a3496dba784fb99935e53c5e98c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/80.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GANILLA(\n",
       "  (D_A): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (D_B): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (G_A): ResNet(\n",
       "    (pad1): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (in1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (pad2): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pad3): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv2): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (tanh): Tanh()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): PyramidFeatures(\n",
       "      (P5_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P4_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P3_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P3_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P2_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P2_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (rp4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (P2_2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (G_B): ResNet(\n",
       "    (pad1): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (in1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (pad2): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (pad3): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (conv2): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (tanh): Tanh()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock_Ganilla(\n",
       "        (rp1): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (rp2): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (shortcut): Sequential()\n",
       "        (final_conv): Sequential(\n",
       "          (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "          (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): PyramidFeatures(\n",
       "      (P5_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P4_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P3_1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P3_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (P2_1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (P2_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (rp4): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (P2_2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "#| output: false\n",
    "ganilla_model.from_pretrained('tmabraham/upit-ganilla-test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
