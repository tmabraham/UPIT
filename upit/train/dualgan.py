# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/08_train.dualgan.ipynb.

# %% auto 0
__all__ = ['DualGANLoss', 'compute_gradient_penalty', 'DualGANTrainer', 'dual_learner']

# %% ../../nbs/08_train.dualgan.ipynb 2
import torch.autograd as autograd
from fastai.vision.all import *
from fastai.basics import *
from typing import List
from fastai.vision.gan import *
from ..models.cyclegan import *
from ..models.dualgan import *
from .cyclegan import *
from ..data.unpaired import *

# %% ../../nbs/08_train.dualgan.ipynb 5
class DualGANLoss(nn.Module):
    """
    DualGAN loss function. The individual loss terms are also atrributes of this class that are accessed by fastai for recording during training.

    Attributes: \n
    `self.dualgan` (`nn.Module`): The DualGAN model. \n
    `self.l_A` (`float`): lambda_A, weight of domain A losses. \n
    `self.l_B` (`float`): lambda_B, weight of domain B losses. \n
    `self.crit` (`AdaptiveLoss`): The adversarial loss function (either a BCE or MSE loss depending on `lsgan` argument) \n
    `self.real_A` and `self.real_B` (`fastai.torch_core.TensorImage`): Real images from domain A and B. \n
    `self.gen_loss` (`torch.FloatTensor`): The generator loss calculated in the forward function \n
    `self.cyc_loss` (`torch.FloatTensor`): The cyclic loss calculated in the forward function
    """

    def __init__(self, dualgan:nn.Module, l_adv:float=1., l_rec:float=1., l_idt:float=0.):
        """
        Constructor for DualGAN loss.

        Arguments:

        `dualgan` (`nn.Module`): The DualGAN model. \n
        `l_adv` (`float`): weight of adversarial losses. (default=1) \n
        `l_rec` (`float`): weight of reconstruction losses. (default=10) \n
        """
        super().__init__()
        store_attr()

    def set_input(self, input): "set `self.real_A` and `self.real_B` for future loss calculation"; self.real_A,self.real_B = input

    def forward(self, output, target):
        """
        Forward function of the DualGAN loss function. The generated images are passed in as output (which comes from the model)
        and the generator loss is returned.
        """
        fake_A, fake_B, idt_A, idt_B = output
        #Generators are trained to trick the discriminators so the following should be ones
        self.adv_loss_A = -torch.mean(self.dualgan.D_A(fake_A)) 
        self.adv_loss_B = -torch.mean(self.dualgan.D_B(fake_B))
  
        #Reconstruction loss
        self.rec_loss_A = F.l1_loss(self.dualgan.G_A(fake_B), self.real_A)
        self.rec_loss_B = F.l1_loss(self.dualgan.G_B(fake_A), self.real_B)
        
        #Identity loss
        self.id_loss_A = F.l1_loss(idt_A, self.real_A)
        self.id_loss_B = F.l1_loss(idt_B, self.real_B)
        
        return self.l_adv*(self.adv_loss_A+self.adv_loss_B)+self.l_rec*(self.rec_loss_A+self.rec_loss_B)+self.l_idt*(self.id_loss_A+self.id_loss_B)

# %% ../../nbs/08_train.dualgan.ipynb 6
def compute_gradient_penalty(D, real_samples, fake_samples):
    """Calculates the gradient penalty loss for WGAN GP"""
    # Random weight term for interpolation between real and fake samples
    alpha = torch.tensor(np.random.random((real_samples.size(0), 1, 1, 1)), device=real_samples.device).float()
    # Get random interpolation between real and fake samples
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    validity = D(interpolates)
    fake = torch.tensor(np.ones(validity.shape), device=real_samples.device).float()
    # Get gradient w.r.t. interpolates
    gradients = autograd.grad(
        outputs=validity,
        inputs=interpolates,
        grad_outputs=fake,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty

# %% ../../nbs/08_train.dualgan.ipynb 7
class DualGANTrainer(Callback):
    """`Learner` Callback for training a DualGAN model."""
    run_before = Recorder
    def __init__(self, n_crit=2, clip_value=0.1, l_gp=None):
        store_attr()
        self.n_disc=0
        self.n_gen=0

        
    def _set_trainable(self, disc=False):
        """Put the generators or discriminators in training mode depending on arguments."""
        def set_requires_grad(m, rg):
            for p in m.parameters(): p.requires_grad_(rg)
        set_requires_grad(self.learn.model.G_A, not disc)
        set_requires_grad(self.learn.model.G_B, not disc)
        set_requires_grad(self.learn.model.D_A, disc)
        set_requires_grad(self.learn.model.D_B, disc)
        if disc: self.opt_D.hypers = self.learn.opt.hypers
        self.gen_mode = not disc
            
                
        
    def before_train(self, **kwargs):
        if not getattr(self,'opt_G',None):
            self.opt_G = self.learn.opt_func(self.learn.splitter(nn.Sequential(*flatten_model(self.G_A), *flatten_model(self.G_B))), self.learn.lr)
        else:
            self.opt_G.hypers = self.learn.opt.hypers
        if not getattr(self, 'opt_D',None):
            self.opt_D = self.learn.opt_func(self.learn.splitter(nn.Sequential(*flatten_model(self.D_A), *flatten_model(self.D_B))), self.learn.lr)
        else:
            self.opt_D.hypers = self.learn.opt.hypers

        self.learn.opt = self.opt_G
        
        self._set_trainable(disc=True)

    def before_batch(self, **kwargs):
        self._training = self.learn.model.training
        self.learn.xb = (self.learn.xb[0],self.learn.yb[0]),
        self.learn.loss_func.set_input(*self.learn.xb)
        if not self.gen_mode:
            self.n_disc += 1
            if self.n_disc == self.n_crit+1:
                self._set_trainable(disc=False)
                self.n_disc=0
            else: return CancelBatchException

                

    def after_pred(self, **kwargs):
        if self._training and not self.gen_mode:
            fake_A, fake_B = self.learn.pred[0].detach(), self.learn.pred[1].detach() 
            (real_A, real_B), = self.learn.xb
            self._set_trainable(disc=True)


            # Adversarial loss
            loss_D_A = -(torch.mean(self.D_A(real_A)) - torch.mean(self.D_A(fake_A)))
            if self.l_gp: # Compute gradient penalty for improved wasserstein training
                gp_A = compute_gradient_penalty(self.D_A, real_A, fake_A)
                loss_D_A += self.l_gp * gp_A
            self.learn.loss_func.D_A_loss = loss_D_A.detach().cpu()

            
            # Adversarial loss
            loss_D_B = -(torch.mean(self.D_B(real_B)) - torch.mean(self.D_B(fake_B))) #
            if self.l_gp: # Compute gradient penalty for improved wasserstein training
                gp_B = compute_gradient_penalty(self.D_B, real_B, fake_B)
                loss_D_B += self.l_gp * gp_B
            self.learn.loss_func.D_B_loss = loss_D_B.detach().cpu()

            loss_D = loss_D_A + loss_D_B
            loss_D.backward()
            self.opt_D.step()
            
            ## Clip weights of discriminators
            if self.clip_value: 
                for p in self.D_A.parameters(): p.data.clamp_(-self.clip_value, self.clip_value)
                for p in self.D_B.parameters(): p.data.clamp_(-self.clip_value, self.clip_value)
            
            self.opt_D.zero_grad()
        else: self.gen_mode = False


# %% ../../nbs/08_train.dualgan.ipynb 8
@delegates(Learner.__init__)
def dual_learner(dls:DataLoader, m:DualGAN, opt_func=RMSProp, loss_func=DualGANLoss, show_imgs:bool=True, imgA:bool=True, imgB:bool=True, show_img_interval:bool=10, metrics:list=[], cbs:list=[], **kwargs):
    """
    Initialize and return a `Learner` object with the data in `dls`, DualGAN model `m`, optimizer function `opt_func`, metrics `metrics`,
    and callbacks `cbs`. Additionally, if `show_imgs` is True, it will show intermediate predictions during training. It will show domain 
    B-to-A predictions if `imgA` is True and/or domain A-to-B predictions if `imgB` is True. Additionally, it will show images every 
    `show_img_interval` epochs. ` Other `Learner` arguments can be passed as well.
    """
    lms = LossMetrics(['adv_loss_A','adv_loss_B','rec_loss_A','rec_loss_B', 
                       'D_A_loss', 'D_B_loss'])
    learn = Learner(dls, m, loss_func=loss_func(m), opt_func=opt_func,
                    cbs=[DualGANTrainer, *cbs],metrics=[*lms, *metrics])
    if (imgA or imgB or show_img_interval) and not show_imgs: warnings.warn('since show_imgs is disabled, ignoring imgA, imgB and show_img_interval arguments')
    if show_imgs: learn.add_cbs(ShowImgsCallback(imgA=imgA,imgB=imgB,show_img_interval=show_img_interval))
    learn.recorder.train_metrics = True
    learn.recorder.valid_metrics = False
    return learn
